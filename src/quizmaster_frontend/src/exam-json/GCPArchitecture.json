{
    "exam_questions": [
        {
            "source": "",
            "topic": "Database Migration to Google Cloud Storage",
            "question": "Your company wants to migrate their 10-TB on-premises database export into Cloud Storage. You want to minimize the time it takes to complete this activity and the overall cost. The bandwidth between the on-premises environment and Google Cloud is 1 Gbps. You want to follow Google-recommended practices. What should you do?",
            "type": "option",
            "choices": {
                "A": "Develop a Dataflow job to read data directly from the database and write it into Cloud Storage.",
                "B": "Use the Data Transfer appliance to perform an offline migration.",
                "C": "Use a commercial partner ETL solution to extract the data from the on-premises database and upload it into Cloud Storage.",
                "D": "Upload the data with gcloud storage cp."
            },
            "keyPoints": [
                {
                    "title": "Bandwidth Consideration",
                    "keywords": [
                        "1 Gbps bandwidth",
                        "data transfer",
                        "minimizing time",
                        "cost-effective"
                    ],
                    "explanation": "With a 1 Gbps connection, uploading a 10-TB database export can be completed efficiently using the `gcloud storage cp` command, which directly uploads files to Cloud Storage, balancing time and cost."
                },
                {
                    "title": "Dataflow and ETL Solutions",
                    "keywords": [
                        "Dataflow",
                        "ETL solutions",
                        "Cloud Storage"
                    ],
                    "explanation": "While Dataflow and ETL solutions are powerful tools for complex data transformations, they may introduce unnecessary complexity and costs for a simple database export. Thus, they might not be the optimal choice for this scenario."
                },
                {
                    "title": "Offline Migration",
                    "keywords": [
                        "Data Transfer appliance",
                        "offline migration",
                        "large datasets"
                    ],
                    "explanation": "The Data Transfer appliance is generally recommended for much larger datasets or when network bandwidth is a significant bottleneck. In this case, a 10-TB transfer over 1 Gbps does not necessitate an offline approach."
                }
            ],
            "answer": "D",
            "page": 1,
            "most_voted": "D",
            "URL": [
                "https://www.examtopics.com/discussions/google/view/121324-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Google Cloud Network Architecture",
            "question": "You are configuring the cloud network architecture for a newly created project in Google Cloud that will host applications in Compute Engine. Compute Engine virtual machine instances will be created in two different subnets (sub-a and sub-b) within a single region:\n• Instances in sub-a will have public IP addresses.\n• Instances in sub-b will have only private IP addresses.\nTo download updated packages, instances must connect to a public repository outside the boundaries of Google Cloud. You need to allow sub-b to access the external repository. What should you do?",
            "type": "option",
            "choices": {
                "A": "Enable Private Google Access on sub-b.",
                "B": "Configure Cloud NAT and select sub-b in the NAT mapping section.",
                "C": "Configure a bastion host instance in sub-a to connect to instances in sub-b.",
                "D": "Enable Identity-Aware Proxy for TCP forwarding for instances in sub-b."
            },
            "keyPoints": [
                {
                    "title": "Private vs. Public IP Access",
                    "keywords": [
                        "subnet",
                        "public IP",
                        "private IP",
                        "external repository"
                    ],
                    "explanation": "Instances in sub-b do not have public IPs and need to access an external repository. To achieve this, a method must be implemented to allow outbound internet access without exposing private IPs directly."
                },
                {
                    "title": "Cloud NAT Configuration",
                    "keywords": [
                        "Cloud NAT",
                        "Network Address Translation",
                        "external access",
                        "Google Cloud"
                    ],
                    "explanation": "Configuring Cloud NAT for sub-b enables the instances to access the internet for updates by translating their private IPs to a public IP, providing secure and controlled external access."
                },
                {
                    "title": "Private Google Access",
                    "keywords": [
                        "Private Google Access",
                        "Google services",
                        "internal traffic"
                    ],
                    "explanation": "Private Google Access allows instances with private IPs to access Google APIs and services, but it does not provide access to external repositories outside Google Cloud."
                }
            ],
            "answer": "B",
            "page": 2,
            "most_voted": "B",
            "URL": [
                "https://www.examtopics.com/discussions/google/view/121322-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Migrating Windows Server Licenses to Google Cloud",
            "question": "Your company is planning to migrate their Windows Server 2022 from their on-premises data center to Google Cloud. You need to bring the licenses that are currently in use in on-premises virtual machines into the target cloud environment. What should you do?",
            "type": "option",
            "choices": {
                "A": "1. Create an image of the on-premises virtual machines and upload into Cloud Storage.\n2. Import the image as a virtual disk on Compute Engine.",
                "B": "1. Create standard instances on Compute Engine.\n2. Select as the OS the same Microsoft Windows version that is currently in use in the on-premises environment.",
                "C": "1. Create an image of the on-premises virtual machine.\n2. Import the image as a virtual disk on Compute Engine.\n3. Create a standard instance on Compute Engine, selecting as the OS the same Microsoft Windows version that is currently in use in the on-premises environment.\n4. Attach a data disk that includes data that matches the created image.",
                "D": "1. Create an image of the on-premises virtual machines.\n2. Import the image as a virtual disk on Compute Engine using --os=windows-2022-dc-v.\n3. Create a sole-tenancy instance on Compute Engine that uses the imported disk as a boot disk."
              },
              "keyPoints": [
                {
                  "title": "Understanding License Portability",
                  "keywords": ["License", "Portability", "Windows Server 2022", "Google Cloud"],
                  "explanation": "In cloud migrations, license portability allows existing software licenses to be brought to the cloud without needing to purchase new licenses. This process often involves creating an image of the existing environment and ensuring compatibility with cloud infrastructure."
                },
                {
                  "title": "Creating and Importing Images",
                  "keywords": ["Virtual Machine Image", "Cloud Storage", "Compute Engine"],
                  "explanation": "Creating a virtual machine image involves capturing the state of a system, including its OS and data. This image can then be imported to cloud services like Google Cloud Compute Engine for deployment."
                },
                {
                  "title": "Sole-Tenancy Instances",
                  "keywords": ["Sole-Tenancy", "Dedicated Resources", "Compute Engine"],
                  "explanation": "A sole-tenancy instance in Google Cloud is a virtual machine that runs on dedicated hardware, which may be necessary for certain licensing requirements or compliance standards."
                }
              ],
            "answer": "D",
            "page": 3,
            "most_voted": "D",
            "URL": [
                "https://www.examtopics.com/discussions/google/view/121314-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Google Cloud VPN and Network Design",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You are deploying an application to Google Cloud. The application is part of a system. The application in Google Cloud must communicate over a private network with applications in a non-Google Cloud environment. The expected average throughput is 200 kbps. The business requires:\n• 99.99% system availability\n• cost optimization\nYou need to design the connectivity between the locations to meet the business requirements. What should you provision?",
            "type": "option",
            "choices": {
                "A": "An HA Cloud VPN gateway connected with two tunnels to an on-premises VPN gateway.",
                "B": "A Classic Cloud VPN gateway connected with two tunnels to an on-premises VPN gateway.",
                "C": "Two HA Cloud VPN gateways connected to two on-premises VPN gateways. Configure each HA Cloud VPN gateway to have two tunnels, each connected to different on-premises VPN gateways.",
                "D": "A Classic Cloud VPN gateway connected with one tunnel to an on-premises VPN gateway."
            },
            "keyPoints": [
                {
                    "title": "High Availability and Redundancy",
                    "keywords": [
                        "HA Cloud VPN",
                        "redundancy",
                        "system availability"
                    ],
                    "explanation": "To achieve 99.99% system availability, using an HA Cloud VPN with multiple tunnels ensures redundancy, minimizing the risk of downtime. The HA configuration is more robust than the classic option."
                },
                {
                    "title": "Cost Optimization",
                    "keywords": [
                        "cost-effective",
                        "VPN",
                        "Cloud VPN"
                    ],
                    "explanation": "While maintaining high availability, an HA Cloud VPN provides a cost-effective solution, avoiding the need for multiple classic VPN gateways while still meeting the business requirements."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 4,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/121313-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Google Cloud Storage and CORS Configuration",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company has an application running on App Engine that allows users to upload music files and share them with other people. You want to allow users to upload files directly into Cloud Storage from their browser session. The payload should not be passed through the backend. What should you do?",
            "type": "option",
            "choices": {
                "A": "1. Set a CORS configuration in the target Cloud Storage bucket where the base URL of the App Engine application is an allowed origin.\n2. Use the Cloud Storage Signed URL feature to generate a POST URL.",
                "B": "1. Set a CORS configuration in the target Cloud Storage bucket where the base URL of the App Engine application is an allowed origin.\n2. Assign the Cloud Storage WRITER role to users who upload files.",
                "C": "1. Use the Cloud Storage Signed URL feature to generate a POST URL.\n2. Use App Engine default credentials to sign requests against Cloud Storage.",
                "D": "1. Assign the Cloud Storage WRITER role to users who upload files.\n2. Use App Engine default credentials to sign requests against Cloud Storage."
              },
              "keyPoints": [
                {
                  "title": "CORS Configuration",
                  "keywords": ["CORS", "Cloud Storage", "Allowed Origin", "App Engine"],
                  "explanation": "Cross-Origin Resource Sharing (CORS) allows your App Engine application to interact with resources in Cloud Storage by specifying the allowed origins and headers. This is crucial for enabling browser-based uploads."
                },
                {
                  "title": "Cloud Storage Signed URL",
                  "keywords": ["Signed URL", "Cloud Storage", "Direct Upload"],
                  "explanation": "Signed URLs allow you to grant limited-time access to Cloud Storage, enabling users to upload files directly without needing to route the data through your backend."
                },
                {
                  "title": "Roles and Permissions",
                  "keywords": ["Cloud Storage WRITER", "Permissions", "App Engine"],
                  "explanation": "Assigning appropriate roles like Cloud Storage WRITER ensures that users have the necessary permissions to upload files directly to Cloud Storage."
                }
              ],
            "answer": "A",
            "most_voted": "A",
            "page": 5,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/121240-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Google Cloud VPN and Network Design",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You are deploying an application to Google Cloud. The application is part of a system. The application in Google Cloud must communicate over a private network with applications in a non-Google Cloud environment. The expected average throughput is 200 kbps. The business requires:\n• as close to 100% system availability as possible\n• cost optimization\nYou need to design the connectivity between the locations to meet the business requirements. What should you provision?",
            "type": "option",
            "choices": {
                "A": "An HA Cloud VPN gateway connected with two tunnels to an on-premises VPN gateway",
                "B": "Two Classic Cloud VPN gateways connected to two on-premises VPN gateways Configure each Classic Cloud VPN gateway to have two tunnels, each connected to different on-premises VPN gateways",
                "C": "Two HA Cloud VPN gateways connected to two on-premises VPN gateways Configure each HA Cloud VPN gateway to have two tunnels, each connected to different on-premises VPN gateways",
                "D": "A single Cloud VPN gateway connected to an on-premises VPN gateway"
            },
            "keyPoints": [
                {
                    "title": "High Availability",
                    "keywords": [
                        "HA Cloud VPN",
                        "redundancy",
                        "system availability"
                    ],
                    "explanation": "Deploying two HA Cloud VPN gateways with multiple tunnels provides maximum redundancy and is essential for achieving close to 100% availability in the system."
                },
                {
                    "title": "Cost Efficiency",
                    "keywords": [
                        "cost optimization",
                        "VPN",
                        "Cloud VPN"
                    ],
                    "explanation": "The use of HA Cloud VPNs over Classic VPNs optimizes costs by providing greater availability and redundancy with fewer resources."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 6,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/80419-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Google Cloud Storage Versioning",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You want to store critical business information in Cloud Storage buckets. The information is regularly changed, but previous versions need to be referenced on a regular basis. You want to ensure that there is a record of all changes to any information in these buckets. You want to ensure that accidental edits or deletions can be easily rolled back. Which feature should you enable?",
            "type": "option",
            "choices": {
                "A": "Bucket Lock",
                "B": "Object Versioning",
                "C": "Object change notification",
                "D": "Object Lifecycle Management"
            },
            "keyPoints": [
                {
                    "title": "Version Control",
                    "keywords": [
                        "Object Versioning",
                        "version control",
                        "data rollback"
                    ],
                    "explanation": "Object Versioning in Cloud Storage allows the retention of multiple versions of an object, enabling you to easily recover or reference previous versions of your critical business data."
                },
                {
                    "title": "Data Protection",
                    "keywords": [
                        "accidental deletion",
                        "data protection",
                        "versioning"
                    ],
                    "explanation": "With Object Versioning, any changes, including deletions, are tracked, providing a safeguard against accidental data loss and ensuring recoverability."
                }
            ],
            "answer": "B",
            "most_voted": "B",
            "page": 7,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/80304-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "BigQuery Cost Monitoring",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company has a Google Cloud project that uses BigQuery for data warehousing on a pay-per-use basis. You want to monitor queries in real time to discover the most costly queries and which users spend the most. What should you do?",
            "type": "option",
            "choices": {
                "A": "1. In the BigQuery dataset that contains all the tables to be queried, add a label for each user that can launch a query.\n2. Open the Billing page of the project.\n3. Select Reports.\n4. Select BigQuery as the product and filter by the user you want to check.",
                "B": "1. Create a Cloud Logging sink to export BigQuery data access logs to BigQuery.\n2. Perform a BigQuery query on the generated table to extract the information you need.",
                "C": "1. Create a Cloud Logging sink to export BigQuery data access logs to Cloud Storage.\n2. Develop a Dataflow pipeline to compute the cost of queries split by users.",
                "D": "1. Activate billing export into BigQuery.\n2. Perform a BigQuery query on the billing table to extract the information you need."
              },
              "keyPoints": [
                {
                  "title": "Billing Monitoring in BigQuery",
                  "keywords": ["BigQuery", "Billing", "Real-time Monitoring"],
                  "explanation": "Monitoring billing in BigQuery helps track the cost of individual queries and identify which users are generating the highest expenses, essential for optimizing cloud costs."
                },
                {
                  "title": "Cloud Logging and Dataflow",
                  "keywords": ["Cloud Logging", "Dataflow", "Cost Analysis"],
                  "explanation": "Using Cloud Logging to export BigQuery logs and Dataflow to analyze them allows for detailed, real-time cost analysis by user and query."
                },
                {
                  "title": "Billing Export",
                  "keywords": ["Billing Export", "BigQuery", "Cost Queries"],
                  "explanation": "Enabling billing export to BigQuery lets you perform detailed queries on billing data, helping in the identification of costly queries and high-spending users."
                }
              ],
            "answer": "B",
            "most_voted": "B",
            "page": 8,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/80112-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Google Cloud Network Integration",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company recently acquired a company that has infrastructure in Google Cloud. Each company has its own Google Cloud organization. Each company is using a Shared Virtual Private Cloud (VPC) to provide network connectivity for its applications. Some of the subnets used by both companies overlap. In order for both businesses to integrate, the applications need to have private network connectivity. These applications are not on overlapping subnets. You want to provide connectivity with minimal re-engineering. What should you do?",
            "type": "option",
            "choices": {
                "A": "Set up VPC peering and peer each Shared VPC together.",
                "B": "Migrate the projects from the acquired company into your company's Google Cloud organization. Re-launch the instances in your companies Shared VPC.",
                "C": "Set up a Cloud VPN gateway in each Shared VPC and peer Cloud VPNs.",
                "D": "Configure SSH port forwarding on each application to provide connectivity between applications in the different Shared VPCs."
            },
            "keyPoints": [
                {
                    "title": "Interconnectivity",
                    "keywords": [
                        "VPC Peering",
                        "Cloud VPN",
                        "network connectivity"
                    ],
                    "explanation": "Using Cloud VPN to connect Shared VPCs from two different Google Cloud organizations allows secure and private communication between applications, even with overlapping subnets."
                },
                {
                    "title": "Minimal Re-engineering",
                    "keywords": [
                        "minimal re-engineering",
                        "VPC",
                        "acquisition integration"
                    ],
                    "explanation": "Cloud VPN offers a straightforward solution for integrating the networks with minimal changes, avoiding the complexities of migrating and reconfiguring existing infrastructures."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 9,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/80075-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Compute Engine Autoscaling",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You have a Compute Engine application that you want to autoscale when total memory usage exceeds 80%. You have installed the Cloud Monitoring agent and configured the autoscaling policy as follows:\n• Metric identifier: agent.googleapis.com/memory/percent_used\n• Filter: metric.label.state = 'used'\n• Target utilization level: 80\n• Target type: GAUGE\nYou observe that the application does not scale under high load. You want to resolve this. What should you do?",
            "type": "option",
            "choices": {
                "A": "Change the Target type to DELTA_PER_MINUTE.",
                "B": "Change the Metric identifier to agent.googleapis.com/memory/bytes_used.",
                "C": "Change the filter to metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND metric.label.state = 'slab'.",
                "D": "Change the filter to metric.label.state = 'free' and the Target utilization to 20."
            },
            "keyPoints": [
                {
                    "title": "Metric Configuration",
                    "keywords": [
                        "autoscaling",
                        "Cloud Monitoring",
                        "metrics"
                    ],
                    "explanation": "The current filter might not be capturing all memory usage states. Modifying the filter to include additional memory states like 'buffered', 'cached', and 'slab' can provide a more accurate representation of memory usage, enabling the autoscaler to function as expected."
                },
                {
                    "title": "Optimizing Autoscaling",
                    "keywords": [
                        "autoscaling policy",
                        "target utilization",
                        "metric accuracy"
                    ],
                    "explanation": "Accurate memory usage metrics are crucial for effective autoscaling. Ensuring that the monitoring agent captures all relevant memory states prevents misconfigurations that could lead to under-scaling or over-scaling."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 10,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/80040-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Google Cloud Shell Usage",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You are managing several projects on Google Cloud and need to interact on a daily basis with BigQuery, Bigtable, and Kubernetes Engine using the gcloud CLI tool. You are traveling a lot and work on different workstations during the week. You want to avoid having to manage the gcloud CLI manually. What should you do?",
            "type": "option",
            "choices": {
                "A": "Use Google Cloud Shell in the Google Cloud Console to interact with Google Cloud.",
                "B": "Create a Compute Engine instance and install gcloud on the instance. Connect to this instance via SSH to always use the same gcloud installation when interacting with Google Cloud.",
                "C": "Install gcloud on all of your workstations. Run the command gcloud components auto-update on each workstation.",
                "D": "Use a package manager to install gcloud on your workstations instead of installing it manually."
            },
            "keyPoints": [
                {
                    "title": "Cloud Shell",
                    "keywords": [
                        "Google Cloud Shell",
                        "gcloud CLI",
                        "cloud console"
                    ],
                    "explanation": "Google Cloud Shell provides a consistent gcloud CLI environment in the cloud that you can access from any browser. It eliminates the need to install and manage the gcloud CLI on multiple workstations."
                },
                {
                    "title": "Mobility",
                    "keywords": [
                        "mobility",
                        "consistent environment",
                        "multi-device"
                    ],
                    "explanation": "Using Google Cloud Shell ensures that you have a consistent CLI environment available across all workstations, making it ideal for users who work from different locations."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 11,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/80035-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Reliable Shutdown Scripts in Compute Engine",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You have a Compute Engine managed instance group that adds and removes Compute Engine instances from the group in response to the load on your application. The instances have a shutdown script that removes REDIS database entries associated with the instance. You see that many database entries have not been removed, and you suspect that the shutdown script is the problem. You need to ensure that the commands in the shutdown script are run reliably every time an instance is shut down. You create a Cloud Function to remove the database entries. What should you do next?",
            "type": "option",
            "choices": {
                "A": "Modify the shutdown script to wait for 30 seconds before triggering the Cloud Function.",
                "B": "Do not use the Cloud Function. Modify the shutdown script to restart if it has not completed in 30 seconds.",
                "C": "Set up a Cloud Monitoring sink that triggers the Cloud Function after an instance removal log message arrives in Cloud Logging.",
                "D": "Modify the shutdown script to wait for 30 seconds and then publish a message to a Pub/Sub queue."
            },
            "keyPoints": [
                {
                    "title": "Ensuring Reliable Execution",
                    "keywords": [
                        "shutdown script",
                        "Cloud Monitoring",
                        "reliable execution"
                    ],
                    "explanation": "By setting up a Cloud Monitoring sink, you can trigger the Cloud Function based on specific logs, ensuring the database entries are removed reliably, even if the shutdown script fails."
                },
                {
                    "title": "Cloud Function Integration",
                    "keywords": [
                        "Cloud Function",
                        "log-based trigger",
                        "reliable automation"
                    ],
                    "explanation": "Integrating Cloud Functions with Cloud Monitoring ensures that critical tasks like database cleanup are performed even if the instance shutdown process is interrupted."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 12,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/80034-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "VPC Peering for Inter-project Communication",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company and one of its partners each have a Google Cloud project in separate organizations. Your company's project (prj-a) runs in Virtual Private Cloud (vpc-a). The partner's project (prj-b) runs in vpc-b. There are two instances running on vpc-a and one instance running on vpc-b. Subnets defined in both VPCs are not overlapping. You need to ensure that all instances communicate with each other via internal IPs, minimizing latency and maximizing throughput. What should you do?",
            "type": "option",
            "choices": {
                "A": "Set up a network peering between vpc-a and vpc-b.",
                "B": "Set up a VPN between vpc-a and vpc-b using Cloud VPN.",
                "C": "Configure IAP TCP forwarding on the instance in vpc-b, and then launch the following gcloud command from one of the instances in vpc-a: gcloud compute start-iap-tunnel INSTANCE_NAME_IN_VPC_8 22 \\ --local-host-port=localhost:22",
                "D": "1. Create an additional instance in vpc-a. 2. Create an additional instance in vpc-b. 3. Install OpenVPN in newly created instances. 4. Configure a VPN tunnel between vpc-a and vpc-b with the help of OpenVPN."
            },
            "keyPoints": [
                {
                    "title": "VPC Peering",
                    "keywords": [
                        "VPC Peering",
                        "low latency",
                        "high throughput"
                    ],
                    "explanation": "VPC Peering allows you to connect two VPC networks for internal IP communication, which minimizes latency and maximizes throughput without the need for additional gateways or VPNs."
                },
                {
                    "title": "Direct Communication",
                    "keywords": [
                        "internal IP",
                        "network peering",
                        "direct communication"
                    ],
                    "explanation": "Setting up VPC Peering is a straightforward and efficient way to enable direct communication between instances in separate VPCs without routing traffic through external networks."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 13,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/80000-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Cost Optimization in Google Kubernetes Engine",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company uses Google Kubernetes Engine (GKE) as a platform for all workloads. Your company has a single large GKE cluster that contains batch, stateful, and stateless workloads. The GKE cluster is configured with a single node pool with 200 nodes. Your company needs to reduce the cost of this cluster but does not want to compromise availability. What should you do?",
            "type": "option",
            "choices": {
                "A": "Create a second GKE cluster for the batch workloads only. Allocate the 200 original nodes across both clusters.",
                "B": "Configure CPU and memory limits on the namespaces in the cluster. Configure all Pods to have CPU and memory limits.",
                "C": "Configure a HorizontalPodAutoscaler for all stateless workloads and for all compatible stateful workloads. Configure the cluster to use node autoscaling.",
                "D": "Change the node pool to use preemptible VMs."
            },
            "keyPoints": [
                {
                    "title": "Autoscaling for Cost Efficiency",
                    "keywords": [
                        "HorizontalPodAutoscaler",
                        "node autoscaling",
                        "cost reduction"
                    ],
                    "explanation": "Using a HorizontalPodAutoscaler and node autoscaling optimizes resource usage by scaling resources up or down based on actual workload demands, reducing costs while maintaining availability."
                },
                {
                    "title": "Resource Limits",
                    "keywords": [
                        "CPU limits",
                        "memory limits",
                        "resource management"
                    ],
                    "explanation": "Setting CPU and memory limits ensures that workloads do not consume more resources than necessary, further contributing to cost savings without impacting performance."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 14,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/79736-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Kubernetes Deployment Strategies",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company has an application running as a Deployment in a Google Kubernetes Engine (GKE) cluster. When releasing new versions of the application via a rolling deployment, the team has been causing outages. The root cause of the outages is misconfigurations with parameters that are only used in production. You want to put preventive measures for this in the platform to prevent outages. What should you do?",
            "type": "option",
            "choices": {
                "A": "Configure liveness and readiness probes in the Pod specification.",
                "B": "Configure health checks on the managed instance group.",
                "C": "Create a Scheduled Task to check whether the application is available.",
                "D": "Configure an uptime alert in Cloud Monitoring."
            },
            "keyPoints": [
                {
                    "title": "Probes for Health Checks",
                    "keywords": [
                        "liveness probe",
                        "readiness probe",
                        "health checks"
                    ],
                    "explanation": "Configuring liveness and readiness probes in the Pod specification ensures that the application is healthy and ready to serve traffic, preventing misconfigurations from causing outages during deployments."
                },
                {
                    "title": "Rolling Deployment Safety",
                    "keywords": [
                        "rolling deployment",
                        "application health",
                        "deployment safety"
                    ],
                    "explanation": "Liveness and readiness probes provide safeguards during rolling deployments by ensuring that only healthy instances are serving traffic, thus preventing outages."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 15,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/79702-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Diagnosing Performance Issues on Compute Engine",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You are managing several internal applications that are deployed on Compute Engine. Business users inform you that an application has become very slow over the past few days. You want to find the underlying cause in order to solve the problem. What should you do first?",
            "type": "option",
            "choices": {
                "A": "Inspect the logs and metrics from the instances in Cloud Logging and Cloud Monitoring.",
                "B": "Change the Compute Engine Instances behind the application to a machine type with more CPU and memory.",
                "C": "Restore a backup of the application database from a time before the application became slow.",
                "D": "Deploy the applications on a managed instance group with autoscaling enabled. Add a load balancer in front of the managed instance group, and have the users connect to the IP of the load balancer."
            },
            "keyPoints": [
                {
                    "title": "Root Cause Analysis",
                    "keywords": [
                        "performance issues",
                        "Cloud Logging",
                        "Cloud Monitoring"
                    ],
                    "explanation": "Using Cloud Logging and Cloud Monitoring to inspect logs and metrics helps identify the root cause of performance issues, allowing you to address the underlying problem effectively."
                },
                {
                    "title": "Effective Troubleshooting",
                    "keywords": [
                        "troubleshooting",
                        "performance diagnostics",
                        "monitoring tools"
                    ],
                    "explanation": "By first analyzing logs and metrics, you can pinpoint the exact issue causing the slowdown, ensuring that any subsequent actions are targeted and effective."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 16,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/79697-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Improving Machine Learning Model Performance",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your customer runs a web service used by e-commerce sites to offer product recommendations to users. The company has begun experimenting with a machine learning model on Google Cloud Platform to improve the quality of results. What should the customer do to improve their model's results over time?",
            "type": "option",
            "choices": {
                "A": "Export Cloud Machine Learning Engine performance metrics from Stackdriver to BigQuery, to be used to analyze the efficiency of the model.",
                "B": "Build a roadmap to move the machine learning model training from Cloud GPUs to Cloud TPUs, which offer better results.",
                "C": "Monitor Compute Engine announcements for availability of newer CPU architectures, and deploy the model to them as soon as they are available for additional performance.",
                "D": "Save a history of recommendations and results of the recommendations in BigQuery, to be used as training data."
            },
            "keyPoints": [
                {
                    "title": "Training Data Collection",
                    "keywords": [
                        "training data",
                        "BigQuery",
                        "model improvement"
                    ],
                    "explanation": "Saving a history of recommendations and their outcomes in BigQuery allows the model to be continuously retrained with updated data, improving its accuracy and effectiveness over time."
                },
                {
                    "title": "Continuous Improvement",
                    "keywords": [
                        "machine learning model",
                        "data-driven improvement",
                        "retraining"
                    ],
                    "explanation": "By regularly updating the training data with real-world results, the machine learning model can be refined to provide better recommendations, adapting to changing patterns."
                }
            ],
            "answer": "D",
            "most_voted": "D",
            "page": 17,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/68718-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Scalable Serverless Compute Options",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You are designing an application for use only during business hours. For the minimum viable product release, you'd like to use a managed product that automatically `scales to zero` so you don't incur costs when there is no activity. Which primary compute resource should you choose?",
            "type": "option",
            "choices": {
                "A": "Cloud Functions",
                "B": "Compute Engine",
                "C": "Google Kubernetes Engine",
                "D": "AppEngine flexible environment"
            },
            "keyPoints": [
                {
                    "title": "Serverless and Cost-Effective",
                    "keywords": [
                        "Cloud Functions",
                        "scales to zero",
                        "serverless"
                    ],
                    "explanation": "Cloud Functions is a serverless compute service that automatically scales to zero when not in use, making it a cost-effective option for applications that are only active during specific times."
                },
                {
                    "title": "Auto-scaling",
                    "keywords": [
                        "auto-scaling",
                        "on-demand compute",
                        "cost management"
                    ],
                    "explanation": "By using a service that scales to zero, you only pay for the compute resources when your application is actively handling requests, reducing costs during inactive periods."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 18,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/68714-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "BigQuery Access Control and Billing",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company is using BigQuery as its enterprise data warehouse. Data is distributed over several Google Cloud projects. All queries on BigQuery need to be billed on a single project. You want to make sure that no query costs are incurred on the projects that contain the data. Users should be able to query the datasets, but not edit them. How should you configure users' access roles?",
            "type": "option",
            "choices": {
                "A": "Add all users to a group. Grant the group the role of BigQuery user on the billing project and BigQuery dataViewer on the projects that contain the data.",
                "B": "Add all users to a group. Grant the group the roles of BigQuery dataViewer on the billing project and BigQuery user on the projects that contain the data.",
                "C": "Add all users to a group. Grant the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects that contain the data.",
                "D": "Add all users to a group. Grant the group the roles of BigQuery dataViewer on the billing project and BigQuery jobUser on the projects that contain the data."
            },
            "keyPoints": [
                {
                    "title": "Role Assignment",
                    "keywords": [
                        "BigQuery roles",
                        "access control",
                        "billing project"
                    ],
                    "explanation": "Assigning the BigQuery jobUser role on the billing project allows users to run queries that are billed to that project while restricting their ability to edit data by granting the BigQuery dataViewer role on the projects containing the data."
                },
                {
                    "title": "Billing Management",
                    "keywords": [
                        "centralized billing",
                        "BigQuery",
                        "role configuration"
                    ],
                    "explanation": "By centralizing billing on a single project, you can control query costs and prevent accidental charges to the projects that only store data."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 19,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/68708-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Data Migration to Google Cloud Storage",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your operations team currently stores 10 TB of data in an object storage service from a third-party provider. They want to move this data to a Cloud Storage bucket as quickly as possible, following Google-recommended practices. They want to minimize the cost of this data migration. Which approach should they use?",
            "type": "option",
            "choices": {
                "A": "Use the gsutil mv command to move the data.",
                "B": "Use the Storage Transfer Service to move the data.",
                "C": "Download the data to a Transfer Appliance, and ship it to Google.",
                "D": "Download the data to the on-premises data center, and upload it to the Cloud Storage bucket."
            },
            "keyPoints": [
                {
                    "title": "Google-recommended Migration",
                    "keywords": [
                        "Storage Transfer Service",
                        "data migration",
                        "Google Cloud Storage"
                    ],
                    "explanation": "The Storage Transfer Service is a Google-recommended tool for efficiently transferring large datasets to Cloud Storage. It minimizes manual effort and cost compared to alternatives."
                },
                {
                    "title": "Cost-effective Transfer",
                    "keywords": [
                        "cost optimization",
                        "data transfer",
                        "cloud migration"
                    ],
                    "explanation": "Using the Storage Transfer Service reduces the need for on-premises infrastructure and provides a streamlined approach to moving large amounts of data with minimal cost."
                }
            ],
            "answer": "B",
            "most_voted": "B",
            "page": 20,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/68693-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "BigQuery Data Access and PII Management",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company has a Google Cloud project that uses BigQuery for data warehousing. There are some tables that contain personally identifiable information (PII). Only the compliance team may access the PII. The other information in the tables must be available to the data science team. You want to minimize cost and the time it takes to assign appropriate access to the tables. What should you do?",
            "type": "option",
            "choices": {
                "A": "1. From the dataset where you have the source data, create views of tables that you want to share, excluding PII. 2. Assign an appropriate project-level IAM role to the members of the data science team. 3. Assign access controls to the dataset that contains the view.",
                "B": "1. From the dataset where you have the source data, create materialized views of tables that you want to share, excluding PII. 2. Assign an appropriate project-level IAM role to the members of the data science team. 3. Assign access controls to the dataset that contains the view.",
                "C": "1. Create a dataset for the data science team. 2. Create views of tables that you want to share, excluding PII. 3. Assign an appropriate project-level IAM role to the members of the data science team. 4. Assign access controls to the dataset that contains the view. 5. Authorize the view to access the source dataset.",
                "D": "1. Create a dataset for the data science team. 2. Create materialized views of tables that you want to share, excluding PII. 3. Assign an appropriate project-level IAM role to the members of the data science team. 4. Assign access controls to the dataset that contains the view. 5. Authorize the view to access the source dataset."
            },
            "keyPoints": [
                {
                    "title": "Data Access Control",
                    "keywords": [
                        "BigQuery",
                        "PII",
                        "IAM roles",
                        "access control"
                    ],
                    "explanation": "Creating views that exclude PII allows the data science team to work with the necessary data without exposing sensitive information. Assigning access controls to these views ensures that only authorized users can access specific data."
                },
                {
                    "title": "Cost and Efficiency",
                    "keywords": [
                        "cost efficiency",
                        "views",
                        "BigQuery access"
                    ],
                    "explanation": "Using views rather than materialized views is more cost-effective and provides a quick way to manage access without the need to duplicate data."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 21,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/68692-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Cloud Bigtable Hotspotting and RowKey Strategy",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company has an application running on Google Cloud that is collecting data from thousands of physical devices that are globally distributed. Data is published to Pub/Sub and streamed in real time into an SSD Cloud Bigtable cluster via a Dataflow pipeline. The operations team informs you that your Cloud Bigtable cluster has a hotspot, and queries are taking longer than expected. You need to resolve the problem and prevent it from happening in the future. What should you do?",
            "type": "option",
            "choices": {
                "A": "Advise your clients to use HBase APIs instead of NodeJS APIs.",
                "B": "Delete records older than 30 days.",
                "C": "Review your RowKey strategy and ensure that keys are evenly spread across the alphabet.",
                "D": "Double the number of nodes you currently have."
            },
            "keyPoints": [
                {
                    "title": "Hotspotting in Cloud Bigtable",
                    "keywords": [
                        "Cloud Bigtable",
                        "hotspotting",
                        "RowKey strategy"
                    ],
                    "explanation": "Hotspotting occurs when too many requests are directed to a single node, which can be mitigated by ensuring that RowKeys are distributed evenly across the cluster."
                },
                {
                    "title": "Optimizing RowKey Strategy",
                    "keywords": [
                        "RowKey",
                        "data distribution",
                        "performance"
                    ],
                    "explanation": "A well-designed RowKey strategy helps distribute data evenly across nodes, preventing hotspots and improving query performance."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 22,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/68691-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Securing IAM Users in Google Cloud Organization",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company has just recently activated Cloud Identity to manage users. The Google Cloud Organization has been configured as well. The security team needs to secure projects that will be part of the Organization. They want to prohibit IAM users outside the domain from gaining permissions from now on. What should they do?",
            "type": "option",
            "choices": {
                "A": "Configure an organization policy to restrict identities by domain.",
                "B": "Configure an organization policy to block creation of service accounts.",
                "C": "Configure Cloud Scheduler to trigger a Cloud Function every hour that removes all users that don't belong to the Cloud Identity domain from all projects.",
                "D": "Create a technical user (e.g., crawler@yourdomain.com), and give it the project owner role at root organization level. Write a bash script that: • Lists all the IAM rules of all projects within the organization. • Deletes all users that do not belong to the company domain. Create a Compute Engine instance in a project within the Organization and configure gcloud to be executed with technical user credentials. Configure a cron job that executes the bash script every hour."
            },
            "keyPoints": [
                {
                    "title": "Restricting Access by Domain",
                    "keywords": [
                        "IAM",
                        "organization policy",
                        "domain restriction"
                    ],
                    "explanation": "Configuring an organization policy to restrict identities by domain is a proactive approach to ensure that only users within the designated domain can access projects within the Google Cloud Organization."
                },
                {
                    "title": "Security Management",
                    "keywords": [
                        "security policy",
                        "IAM restrictions",
                        "Google Cloud Organization"
                    ],
                    "explanation": "Restricting access based on domain prevents unauthorized users from gaining permissions, enhancing the overall security posture of the organization."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 23,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/68690-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Log Storage for Compliance",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company has an application that is running on multiple instances of Compute Engine. It generates 1 TB per day of logs. For compliance reasons, the logs need to be kept for at least two years. The logs need to be available for active query for 30 days. After that, they just need to be retained for audit purposes. You want to implement a storage solution that is compliant, minimizes costs, and follows Google-recommended practices. What should you do?",
            "type": "option",
            "choices": {
                "A": "1. Install a Cloud Logging agent on all instances. 2. Create a sink to export logs into a regional Cloud Storage bucket. 3. Create an Object Lifecycle rule to move files into a Coldline Cloud Storage bucket after one month. 4. Configure a retention policy at the bucket level using bucket lock.",
                "B": "1. Write a daily cron job, running on all instances, that uploads logs into a Cloud Storage bucket. 2. Create a sink to export logs into a regional Cloud Storage bucket. 3. Create an Object Lifecycle rule to move files into a Coldline Cloud Storage bucket after one month.",
                "C": "1. Install a Cloud Logging agent on all instances. 2. Create a sink to export logs into a partitioned BigQuery table. 3. Set a time_partitioning_expiration of 30 days.",
                "D": "1. Create a daily cron job, running on all instances, that uploads logs into a partitioned BigQuery table. 2. Set a time_partitioning_expiration of 30 days."
            },
            "keyPoints": [
                {
                    "title": "Cost-Effective Log Storage",
                    "keywords": [
                        "Cloud Logging",
                        "Coldline Storage",
                        "Object Lifecycle"
                    ],
                    "explanation": "Using Cloud Storage with Object Lifecycle management allows logs to be stored in a cost-effective manner by moving older logs to Coldline Storage, which is designed for long-term storage with lower access frequency."
                },
                {
                    "title": "Compliance and Retention",
                    "keywords": [
                        "data retention",
                        "compliance",
                        "log storage"
                    ],
                    "explanation": "Configuring a retention policy with bucket lock ensures that logs are retained for the required period, meeting compliance requirements while minimizing storage costs."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 24,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/68688-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Log Aggregation for Production Projects",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You want to allow your operations team to store logs from all the production projects in your Organization, without including logs from other projects. All of the production projects are contained in a folder. You want to ensure that all logs for existing and new production projects are captured automatically. What should you do?",
            "type": "option",
            "choices": {
                "A": "Create an aggregated export on the Production folder. Set the log sink to be a Cloud Storage bucket in an operations project.",
                "B": "Create an aggregated export on the Organization resource. Set the log sink to be a Cloud Storage bucket in an operations project.",
                "C": "Create log exports in the production projects. Set the log sinks to be a Cloud Storage bucket in an operations project.",
                "D": "Create log exports in the production projects. Set the log sinks to be BigQuery datasets in the production projects, and grant IAM access to the operations team to run queries on the datasets."
            },
            "keyPoints": [
                {
                    "title": "Aggregated Export for Logs",
                    "keywords": [
                        "log aggregation",
                        "production logs",
                        "Cloud Storage"
                    ],
                    "explanation": "Using an aggregated export on the Production folder ensures that all logs from production projects are captured and stored in a centralized location, making it easier to manage and analyze logs."
                },
                {
                    "title": "Automated Log Capture",
                    "keywords": [
                        "log automation",
                        "Cloud Logging",
                        "operations management"
                    ],
                    "explanation": "Setting up an aggregated export automates the process of capturing logs for all current and future production projects, ensuring no logs are missed."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 25,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/68686-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Processing External Data Without Storing PII",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You are working with a data warehousing team that performs data analysis. The team needs to process data from external partners, but the data contains personally identifiable information (PII). You need to process and store the data without storing any of the PII data. What should you do?",
            "type": "option",
            "choices": {
                "A": "Create a Dataflow pipeline to retrieve the data from the external sources. As part of the pipeline, use the Cloud Data Loss Prevention (Cloud DLP) API to remove any PII data. Store the result in BigQuery.",
                "B": "Create a Dataflow pipeline to retrieve the data from the external sources. As part of the pipeline, store all non-PII data in BigQuery and store all PII data in a Cloud Storage bucket that has a retention policy set.",
                "C": "Ask the external partners to upload all data on Cloud Storage. Configure Bucket Lock for the bucket. Create a Dataflow pipeline to read the data from the bucket. As part of the pipeline, use the Cloud Data Loss Prevention (Cloud DLP) API to remove any PII data. Store the result in BigQuery.",
                "D": "Ask the external partners to import all data in your BigQuery dataset. Create a dataflow pipeline to copy the data into a new table. As part of the Dataflow bucket, skip all data in columns that have PII data"
            },
            "keyPoints": [
                {
                    "title": "PII Removal",
                    "keywords": [
                        "Cloud DLP",
                        "PII",
                        "data processing"
                    ],
                    "explanation": "Using the Cloud Data Loss Prevention (DLP) API in a Dataflow pipeline allows you to process and store data from external partners while ensuring that PII is removed, complying with data protection regulations."
                },
                {
                    "title": "Secure Data Processing",
                    "keywords": [
                        "secure data",
                        "BigQuery",
                        "Dataflow"
                    ],
                    "explanation": "By processing data to remove PII before storage, you ensure that the resulting data is secure and compliant, allowing for safe analysis in BigQuery."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 26,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/68685-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Saving Cloud VPN Logs for Compliance",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "The operations team in your company wants to save Cloud VPN log events for one year. You need to configure the cloud infrastructure to save the logs. What should you do?",
            "type": "option",
            "choices": {
                "A": "Set up a filter in Cloud Logging and a Cloud Storage bucket as an export target for the logs you want to save.",
                "B": "Enable the Compute Engine API, and then enable logging on the firewall rules that match the traffic you want to save.",
                "C": "Set up a Cloud Logging Dashboard titled Cloud VPN Logs, and then add a chart that queries for the VPN metrics over a one-year time period.",
                "D": "Set up a filter in Cloud Logging and a topic in Pub/Sub to publish the logs."
            },
            "keyPoints": [
                {
                    "title": "Long-Term Log Storage",
                    "keywords": [
                        "Cloud Logging",
                        "log retention",
                        "Cloud VPN"
                    ],
                    "explanation": "Using Cloud Logging filters to export logs to Cloud Storage allows you to store VPN logs for an extended period, ensuring compliance with data retention policies."
                },
                {
                    "title": "Compliance with Log Retention",
                    "keywords": [
                        "log retention",
                        "compliance",
                        "Cloud Storage"
                    ],
                    "explanation": "Storing logs in Cloud Storage with the appropriate retention settings ensures that your company meets compliance requirements while maintaining access to historical data."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 27,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/68684-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Improving Performance in Cloud Storage",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company has an application running on Compute Engine that allows users to play their favorite music. There are a fixed number of instances. Files are stored in Cloud Storage, and data is streamed directly to users. Users are reporting that they sometimes need to attempt to play popular songs multiple times before they are successful. You need to improve the performance of the application. What should you do?",
            "type": "option",
            "choices": {
                "A": "1. Mount the Cloud Storage bucket using gcsfuse on all backend Compute Engine instances. 2. Serve music files directly from the backend Compute Engine instance.",
                "B": "1. Create a Cloud Filestore NFS volume and attach it to the backend Compute Engine instances. 2. Download popular songs in Cloud Filestore. 3. Serve music files directly from the backend Compute Engine instance.",
                "C": "1. Copy popular songs into CloudSQL as a blob. 2. Update application code to retrieve data from CloudSQL when Cloud Storage is overloaded.",
                "D": "1. Create a managed instance group with Compute Engine instances. 2. Create a global load balancer and configure it with two backends: • Managed instance group • Cloud Storage bucket 3. Enable Cloud CDN on the bucket backend."
            },
            "keyPoints": [
                {
                    "title": "Improving Content Delivery",
                    "keywords": [
                        "Cloud Storage",
                        "performance",
                        "Cloud CDN"
                    ],
                    "explanation": "Enabling Cloud CDN on the Cloud Storage backend ensures that popular content is cached closer to users, reducing latency and improving the playback experience for users."
                },
                {
                    "title": "Scalability",
                    "keywords": [
                        "scalability",
                        "global load balancer",
                        "Compute Engine"
                    ],
                    "explanation": "Using a managed instance group with a global load balancer distributes the load efficiently across multiple instances, ensuring high availability and improved performance."
                }
            ],
            "answer": "D",
            "most_voted": "D",
            "page": 28,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/68683-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Google Cloud Organization Structure and Policy Management",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company has a Google Workspace account and Google Cloud Organization. Some developers in the company have created Google Cloud projects outside of the Google Cloud Organization. You want to create an Organization structure that allows developers to create projects, but prevents them from modifying production projects. You want to manage policies for all projects centrally and be able to set more restrictive policies for production projects. You want to minimize disruption to users and developers when business needs change in the future. You want to follow Google-recommended practices. Now should you design the Organization structure?",
            "type": "option",
            "choices": {
                "A": "1. Create a second Google Workspace account and Organization. 2. Grant all developers the Project Creator IAM role on the new Organization. 3. Move the developer projects into the new Organization. 4. Set the policies for all projects on both Organizations. 5. Additionally, set the production policies on the original Organization.",
                "B": "1. Create a folder under the Organization resource named 'Production.' 2. Grant all developers the Project Creator IAM role on the new Organization. 3. Move the developer projects into the new Organization. 4. Set the policies for all projects on the Organization. 5. Additionally, set the production policies on the 'Production' folder.",
                "C": "1. Create folders under the Organization resource named 'Development' and 'Production.' 2. Grant all developers the Project Creator IAM role on the 'Development' folder. 3. Move the developer projects into the 'Development' folder. 4. Set the policies for all projects on the Organization. 5. Additionally, set the production policies on the 'Production' folder.",
                "D": "1. Designate the Organization for production projects only. 2. Ensure that developers do not have the Project Creator IAM role on the Organization. 3. Create development projects outside of the Organization using the developer Google Workspace accounts. 4. Set the policies for all projects on the Organization. 5. Additionally, set the production policies on the individual production projects."
            },
            "keyPoints": [
                {
                    "title": "Centralized Policy Management",
                    "keywords": [
                        "Organization structure",
                        "IAM roles",
                        "policy management"
                    ],
                    "explanation": "Creating separate folders for 'Development' and 'Production' allows you to set specific policies for each environment, ensuring that developers can create projects without impacting production."
                },
                {
                    "title": "Google-Recommended Practices",
                    "keywords": [
                        "best practices",
                        "Organization management",
                        "policy enforcement"
                    ],
                    "explanation": "Following Google-recommended practices by using folders under the Organization resource allows for a scalable and manageable structure that can adapt to changing business needs with minimal disruption."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 29,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/68682-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Tokenization and Data Encryption",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "For this question, refer to the Helicopter Racing League (HRL) case study. Your team is in charge of creating a payment card data vault for card numbers used to bill tens of thousands of viewers, merchandise consumers, and season ticket holders. You need to implement a custom card tokenization service that meets the following requirements:\n* It must provide low latency at minimal cost.\n* It must be able to identify duplicate credit cards and must not store plaintext card numbers.\n* It should support annual key rotation.\nWhich storage approach should you adopt for your tokenization service?",
            "type": "option",
            "choices": {
                "A": "Store the card data in Secret Manager after running a query to identify duplicates.",
                "B": "Encrypt the card data with a deterministic algorithm stored in Firestore using Datastore mode.",
                "C": "Encrypt the card data with a deterministic algorithm and shard it across multiple Memorystore instances.",
                "D": "Use column-level encryption to store the data in Cloud SQL."
            },
            "keyPoints": [
                {
                    "title": "Tokenization and Low Latency",
                    "keywords": [
                        "tokenization",
                        "data encryption",
                        "low latency"
                    ],
                    "explanation": "Encrypting card data with a deterministic algorithm in Firestore (Datastore mode) allows for low-latency operations and supports tokenization while ensuring that plaintext card numbers are not stored."
                },
                {
                    "title": "Data Security",
                    "keywords": [
                        "security",
                        "encryption",
                        "Firestore"
                    ],
                    "explanation": "Using a deterministic encryption algorithm ensures that the same input (e.g., a card number) always produces the same token, which is crucial for identifying duplicates while maintaining security."
                }
            ],
            "answer": "B",
            "most_voted": "B",
            "page": 30,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/65937-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Google Cloud IAM Best Practices",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You are responsible for the Google Cloud environment in your company. Multiple departments need access to their own projects, and the members within each department will have the same project responsibilities. You want to structure your Google Cloud environment for minimal maintenance and maximum overview of IAM permissions as each department's projects start and end. You want to follow Google-recommended practices. What should you do?",
            "type": "option",
            "choices": {
                "A": "Grant all department members the required IAM permissions for their respective projects.",
                "B": "Create a Google Group per department and add all department members to their respective groups. Create a folder per department and grant the respective group the required IAM permissions at the folder level. Add the projects under the respective folders.",
                "C": "Create a folder per department and grant the respective members of the department the required IAM permissions at the folder level. Structure all projects for each department under the respective folders.",
                "D": "Create a Google Group per department and add all department members to their respective groups. Grant each group the required IAM permissions for their respective projects."
            },
            "keyPoints": [
                {
                    "title": "IAM Permissions Management",
                    "keywords": [
                        "IAM",
                        "Google Groups",
                        "folder structure"
                    ],
                    "explanation": "Using Google Groups and assigning IAM permissions at the folder level reduces maintenance overhead and provides a centralized way to manage access, ensuring that permissions are consistently applied as new projects are created."
                },
                {
                    "title": "Scalability and Best Practices",
                    "keywords": [
                        "best practices",
                        "scalable access management",
                        "Google Cloud"
                    ],
                    "explanation": "Google recommends structuring IAM permissions at the folder level using groups, as it allows for better scalability and easier management as projects start and end."
                }
            ],
            "answer": "B",
            "most_voted": "B",
            "page": 31,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60743-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Google Cloud Data Migration",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company wants to migrate their 10-TB on-premises database export into Cloud Storage. You want to minimize the time it takes to complete this activity, the overall cost, and database load. The bandwidth between the on-premises environment and Google Cloud is 1 Gbps. You want to follow Google-recommended practices. What should you do?",
            "type": "option",
            "choices": {
                "A": "Develop a Dataflow job to read data directly from the database and write it into Cloud Storage.",
                "B": "Use the Data Transfer appliance to perform an offline migration.",
                "C": "Use a commercial partner ETL solution to extract the data from the on-premises database and upload it into Cloud Storage.",
                "D": "Compress the data and upload it with gsutil -m to enable multi-threaded copy."
            },
            "keyPoints": [
                {
                    "title": "Offline Data Transfer",
                    "keywords": [
                        "Data Transfer appliance",
                        "offline migration",
                        "large data transfer"
                    ],
                    "explanation": "Using the Data Transfer appliance is a Google-recommended method for migrating large datasets offline, reducing the impact on the network and minimizing the database load during the transfer process."
                },
                {
                    "title": "Efficiency and Cost Optimization",
                    "keywords": [
                        "cost efficiency",
                        "data migration",
                        "network bandwidth"
                    ],
                    "explanation": "By performing an offline migration, you minimize the time and cost associated with transferring large amounts of data over limited network bandwidth."
                }
            ],
            "answer": "B",
            "most_voted": "B",
            "page": 32,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60720-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Web Application Auditing",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company is developing a web-based application. You need to make sure that production deployments are linked to source code commits and are fully auditable. What should you do?",
            "type": "option",
            "choices": {
                "A": "Make sure a developer is tagging the code commit with the date and time of commit.",
                "B": "Make sure a developer is adding a comment to the commit that links to the deployment.",
                "C": "Make the container tag match the source code commit hash.",
                "D": "Make sure the developer is tagging the commits with latest."
            },
            "keyPoints": [
                {
                    "title": "Commit Tracking",
                    "keywords": [
                        "source code",
                        "commit hash",
                        "container tag"
                    ],
                    "explanation": "Linking production deployments to source code commits via the commit hash ensures that every deployment is auditable, providing a clear trace between the code and the deployed application."
                },
                {
                    "title": "Auditability",
                    "keywords": [
                        "audit",
                        "deployment tracking",
                        "CI/CD"
                    ],
                    "explanation": "Using the commit hash as a tag for container images allows teams to easily trace back and review the specific code that was deployed, which is crucial for auditing and debugging."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 33,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60698-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Data Lake Ingestion and Processing",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company is designing its data lake on Google Cloud and wants to develop different ingestion pipelines to collect unstructured data from different sources. After the data is stored in Google Cloud, it will be processed in several data pipelines to build a recommendation engine for end users on the website. The structure of the data retrieved from the source systems can change at any time. The data must be stored exactly as it was retrieved for reprocessing purposes in case the data structure is incompatible with the current processing pipelines. You need to design an architecture to support the use case after you retrieve the data. What should you do?",
            "type": "option",
            "choices": {
                "A": "Send the data through the processing pipeline, and then store the processed data in a BigQuery table for reprocessing.",
                "B": "Store the data in a BigQuery table. Design the processing pipelines to retrieve the data from the table.",
                "C": "Send the data through the processing pipeline, and then store the processed data in a Cloud Storage bucket for reprocessing.",
                "D": "Store the data in a Cloud Storage bucket. Design the processing pipelines to retrieve the data from the bucket."
            },
            "keyPoints": [
                {
                    "title": "Data Storage Flexibility",
                    "keywords": [
                        "data lake",
                        "unstructured data",
                        "Cloud Storage"
                    ],
                    "explanation": "Storing unstructured data in Cloud Storage allows you to retain the original data exactly as it was retrieved, enabling reprocessing if the data structure changes or if processing pipelines need to be updated."
                },
                {
                    "title": "Processing Pipeline Integration",
                    "keywords": [
                        "data pipelines",
                        "reprocessing",
                        "Cloud Storage"
                    ],
                    "explanation": "Designing processing pipelines to retrieve data from Cloud Storage ensures that the data lake can handle schema changes and still retain the ability to reprocess historical data if needed."
                }
            ],
            "answer": "D",
            "most_voted": "D",
            "page": 34,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60682-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Reducing Latency with GKE",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company has a stateless web API that performs scientific calculations. The web API runs on a single Google Kubernetes Engine (GKE) cluster. The cluster is currently deployed in us-central1. Your company has expanded to offer your API to customers in Asia. You want to reduce the latency for users in Asia. What should you do?",
            "type": "option",
            "choices": {
                "A": "Create a second GKE cluster in asia-southeast1, and expose both APIs using a Service of type LoadBalancer. Add the public IPs to the Cloud DNS zone.",
                "B": "Use a global HTTP(s) load balancer with Cloud CDN enabled.",
                "C": "Create a second GKE cluster in asia-southeast1, and use kubemci to create a global HTTP(s) load balancer.",
                "D": "Increase the memory and CPU allocated to the application in the cluster."
            },
            "keyPoints": [
                {
                    "title": "Global Load Balancing",
                    "keywords": [
                        "global load balancer",
                        "GKE",
                        "latency reduction"
                    ],
                    "explanation": "Using kubemci to create a global HTTP(s) load balancer across multiple GKE clusters allows traffic to be routed to the nearest cluster, reducing latency for users in different regions."
                },
                {
                    "title": "Regional Deployment",
                    "keywords": [
                        "GKE",
                        "regional deployment",
                        "latency"
                    ],
                    "explanation": "Deploying additional GKE clusters in regions closer to your users ensures that requests are handled more quickly, improving performance and user experience."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 35,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60627-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "SRE Practices and Anthos",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You have deployed an application on Anthos clusters (formerly Anthos GKE). According to the SRE practices at your company, you need to be alerted if request latency is above a certain threshold for a specified amount of time. What should you do?",
            "type": "option",
            "choices": {
                "A": "Install Anthos Service Mesh on your cluster. Use the Google Cloud Console to define a Service Level Objective (SLO), and create an alerting policy based on this SLO.",
                "B": "Enable the Cloud Trace API on your project, and use Cloud Monitoring Alerts to send an alert based on the Cloud Trace metrics.",
                "C": "Use Cloud Profiler to follow up the request latency. Create a custom metric in Cloud Monitoring based on the results of Cloud Profiler, and create an Alerting policy in case this metric exceeds the threshold.",
                "D": "Configure Anthos Config Management on your cluster, and create a yaml file that defines the SLO and alerting policy you want to deploy in your cluster."
            },
            "keyPoints": [
                {
                    "title": "Service Level Objectives (SLOs)",
                    "keywords": [
                        "SRE",
                        "SLO",
                        "Anthos"
                    ],
                    "explanation": "Defining an SLO in Anthos Service Mesh allows you to monitor and alert on the performance of your services, ensuring that any latency issues are addressed promptly according to your SRE practices."
                },
                {
                    "title": "Proactive Monitoring",
                    "keywords": [
                        "alerting",
                        "Cloud Monitoring",
                        "latency"
                    ],
                    "explanation": "Creating an alerting policy based on SLOs ensures that your operations team is notified when the service performance degrades, allowing for quick resolution of potential issues."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 36,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60624-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "High Availability in Compute Engine",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company has an enterprise application running on Compute Engine that requires high availability and high performance. The application has been deployed on two instances in two zones in the same region in active-passive mode. The application writes data to a persistent disk. In the case of a single zone outage, that data should be immediately made available to the other instance in the other zone. You want to maximize performance while minimizing downtime and data loss. What should you do?",
            "type": "option",
            "choices": {
                "A": "1. Attach a persistent SSD disk to the first instance. 2. Create a snapshot every hour. 3. In case of a zone outage, recreate a persistent SSD disk in the second instance where data is coming from the created snapshot.",
                "B": "1. Create a Cloud Storage bucket. 2. Mount the bucket into the first instance with gcs-fuse. 3. In case of a zone outage, mount the Cloud Storage bucket to the second instance with gcs-fuse.",
                "C": "1. Attach a regional SSD persistent disk to the first instance. 2. In case of a zone outage, force-attach the disk to the other instance.",
                "D": "1. Attach a local SSD to the first instance disk. 2. Execute an rsync command every hour where the target is a persistent SSD disk attached to the second instance. 3. In case of a zone outage, use the second instance."
            },
            "keyPoints": [
                {
                    "title": "Regional Persistent Disk",
                    "keywords": [
                        "regional SSD",
                        "high availability",
                        "Compute Engine"
                    ],
                    "explanation": "Using a regional SSD persistent disk ensures that data is replicated across multiple zones, providing high availability and minimizing the risk of data loss during a zone outage."
                },
                {
                    "title": "Active-Passive Failover",
                    "keywords": [
                        "failover",
                        "zone outage",
                        "Compute Engine"
                    ],
                    "explanation": "In an active-passive setup, using a regional disk allows the passive instance to take over with minimal downtime, as the data is already synchronized between the two zones."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 37,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60583-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Continuous Integration with Google Cloud Build",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "For this question, refer to the TerramEarth case study. You are building a microservice-based application for TerramEarth. The application is based on Docker containers. You want to follow Google-recommended practices to build the application continuously and store the build artifacts. What should you do?",
            "type": "option",
            "choices": {
                "A": "Configure a trigger in Cloud Build for new source changes. Invoke Cloud Build to build container images for each microservice, and tag them using the code commit hash. Push the images to the Container Registry.",
                "B": "Configure a trigger in Cloud Build for new source changes. The trigger invokes build jobs and builds container images for the microservices. Tag the images with a version number, and push them to Cloud Storage.",
                "C": "Create a Scheduler job to check the repo every minute. For any new change, invoke Cloud Build to build container images for the microservices. Tag the images using the current timestamp, and push them to the Container Registry.",
                "D": "Configure a trigger in Cloud Build for new source changes. Invoke Cloud Build to build one container image, and tag the image with the label 'latest.' Push the image to the Container Registry."
            },
            "keyPoints": [
                {
                    "title": "Automated CI/CD Pipeline",
                    "keywords": [
                        "CI/CD",
                        "Cloud Build",
                        "microservices"
                    ],
                    "explanation": "Using Cloud Build with triggers based on source changes ensures that container images for each microservice are built and stored automatically, following best practices for continuous integration."
                },
                {
                    "title": "Version Control",
                    "keywords": [
                        "versioning",
                        "container images",
                        "code commit hash"
                    ],
                    "explanation": "Tagging images with the code commit hash provides a clear and traceable link between the source code and the deployed application, supporting auditing and rollback processes."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 38,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60563-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Monitoring Legacy Applications",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "For this question, refer to the TerramEarth case study. TerramEarth has a legacy web application that you cannot migrate to cloud. However, you still want to build a cloud-native way to monitor the application. If the application goes down, you want the URL to point to a 'Site is unavailable' page as soon as possible. You also want your Ops team to receive a notification for the issue. You need to build a reliable solution for minimum cost. What should you do?",
            "type": "option",
            "choices": {
                "A": "Create a scheduled job in Cloud Run to invoke a container every minute. The container will check the application URL. If the application is down, switch the URL to the 'Site is unavailable' page, and notify the Ops team.",
                "B": "Create a cron job on a Compute Engine VM that runs every minute. The cron job invokes a Python program to check the application URL. If the application is down, switch the URL to the 'Site is unavailable' page, and notify the Ops team.",
                "C": "Create a Cloud Monitoring uptime check to validate the application URL. If it fails, put a message in a Pub/Sub queue that triggers a Cloud Function to switch the URL to the 'Site is unavailable' page, and notify the Ops team.",
                "D": "Use Cloud Error Reporting to check the application URL. If the application is down, switch the URL to the 'Site is unavailable' page, and notify the Ops team."
            },
            "keyPoints": [
                {
                    "title": "Cloud-Native Monitoring",
                    "keywords": [
                        "Cloud Monitoring",
                        "uptime check",
                        "legacy application"
                    ],
                    "explanation": "Setting up a Cloud Monitoring uptime check allows you to monitor the availability of a legacy application and respond automatically by updating the URL and notifying the operations team when an issue occurs."
                },
                {
                    "title": "Cost-Effective Solution",
                    "keywords": [
                        "cost management",
                        "cloud-native tools",
                        "monitoring"
                    ],
                    "explanation": "Using Cloud Monitoring and Cloud Functions provides a reliable, automated, and cost-effective solution for monitoring and managing legacy applications without requiring significant infrastructure."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 39,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60562-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Handling CVEs in Google Cloud",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "For this question, refer to the TerramEarth case study. You are migrating a Linux-based application from your private data center to Google Cloud. The TerramEarth security team sent you several recent Linux vulnerabilities published by Common Vulnerabilities and Exposures (CVE). You need assistance in understanding how these vulnerabilities could impact your migration. What should you do? (Choose two.)",
            "type": "multiple_choice",
            "choices": {
                "A": "Open a support case regarding the CVE and chat with the support engineer.",
                "B": "Read the CVEs from the Google Cloud Status Dashboard to understand the impact.",
                "C": "Read the CVEs from the Google Cloud Platform Security Bulletins to understand the impact.",
                "D": "Post a question regarding the CVE in Stack Overflow to get an explanation.",
                "E": "Post a question regarding the CVE in a Google Cloud discussion group to get an explanation."
            },
            "keyPoints": [
                {
                    "title": "Security Vulnerabilities",
                    "keywords": [
                        "CVE",
                        "security",
                        "Google Cloud Platform"
                    ],
                    "explanation": "Understanding the impact of security vulnerabilities on your Google Cloud environment is crucial for maintaining a secure deployment. Utilizing Google Cloud resources, such as the Security Bulletins, helps in assessing these vulnerabilities."
                },
                {
                    "title": "Support and Best Practices",
                    "keywords": [
                        "support case",
                        "CVE impact",
                        "cloud migration"
                    ],
                    "explanation": "Opening a support case with Google Cloud provides direct access to expert guidance, helping to understand the specific impact of CVEs on your migration and ensuring that best practices are followed."
                }
            ],
            "answer": [
                "A",
                "C"
            ],
            "most_voted": "A,C",
            "page": 40,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60557-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "High Availability for Microservices on Cloud Run",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "For this question, refer to the TerramEarth case study. You have broken down a legacy monolithic application into a few containerized RESTful microservices. You want to run those microservices on Cloud Run. You also want to make sure the services are highly available with low latency to your customers. What should you do?",
            "type": "option",
            "choices": {
                "A": "Deploy Cloud Run services to multiple availability zones. Create Cloud Endpoints that point to the services. Create a global HTTP(S) Load Balancing instance and attach the Cloud Endpoints to its backend.",
                "B": "Deploy Cloud Run services to multiple regions. Create serverless network endpoint groups pointing to the services. Add the serverless NEGs to a backend service that is used by a global HTTP(S) Load Balancing instance.",
                "C": "Deploy Cloud Run services to multiple regions. In Cloud DNS, create a latency-based DNS name that points to the services.",
                "D": "Deploy Cloud Run services to multiple availability zones. Create a TCP/IP global load balancer. Add the Cloud Run Endpoints to its backend service."
            },
            "keyPoints": [
                {
                    "title": "High Availability and Low Latency",
                    "keywords": [
                        "Cloud Run",
                        "global load balancer",
                        "serverless NEGs"
                    ],
                    "explanation": "Deploying Cloud Run services to multiple regions and using serverless network endpoint groups (NEGs) with a global HTTP(S) load balancer ensures high availability and low latency by routing traffic to the closest available region."
                },
                {
                    "title": "Scalable and Resilient Architecture",
                    "keywords": [
                        "serverless",
                        "multi-region",
                        "load balancing"
                    ],
                    "explanation": "Using a global HTTP(S) load balancer with serverless NEGs allows for automatic scaling and resilience, distributing traffic based on proximity and availability."
                }
            ],
            "answer": "B",
            "most_voted": "B",
            "page": 41,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60525-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Secure Function Invocation in Google Cloud",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "For this question, refer to the TerramEarth case study. You start to build a new application that uses a few Cloud Functions for the backend. One use case requires a Cloud Function `func_display` to invoke another Cloud Function `func_query`. You want `func_query` only to accept invocations from `func_display`. You also want to follow Google's recommended best practices. What should you do?",
            "type": "option",
            "choices": {
                "A": "Create a token and pass it in as an environment variable to `func_display`. When invoking `func_query`, include the token in the request. Pass the same token to `func_query` and reject the invocation if the tokens are different.",
                "B": "Make `func_query` 'Require authentication.' Create a unique service account and associate it to `func_display`. Grant the service account invoker role for `func_query`. Create an ID token in `func_display` and include the token to the request when invoking `func_query`.",
                "C": "Make `func_query` 'Require authentication' and only accept internal traffic. Create those two functions in the same VPC. Create an ingress firewall rule for `func_query` to only allow traffic from `func_display`.",
                "D": "Create those two functions in the same project and VPC. Make `func_query` only accept internal traffic. Create an ingress firewall for `func_query` to only allow traffic from `func_display`. Also, make sure both functions use the same service account."
            },
            "keyPoints": [
                {
                    "title": "Authenticated Function Invocation",
                    "keywords": [
                        "Cloud Functions",
                        "service account",
                        "secure invocation"
                    ],
                    "explanation": "Using service accounts and requiring authentication ensures that only authorized functions can invoke others, following Google’s best practices for securing serverless applications."
                },
                {
                    "title": "Best Practices in Security",
                    "keywords": [
                        "best practices",
                        "function security",
                        "service accounts"
                    ],
                    "explanation": "By creating a unique service account and using ID tokens for secure invocation, you minimize the risk of unauthorized access and ensure that function interactions are secure."
                }
            ],
            "answer": "B",
            "most_voted": "B",
            "page": 42,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60524-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Deploying Workloads on Sole-Tenant Nodes",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You are working at an institution that processes medical data. You are migrating several workloads onto Google Cloud. Company policies require all workloads to run on physically separated hardware, and workloads from different clients must also be separated. You created a sole-tenant node group and added a node for each client. You need to deploy the workloads on these dedicated hosts. What should you do?",
            "type": "option",
            "choices": {
                "A": "Add the node group name as a network tag when creating Compute Engine instances in order to host each workload on the correct node group.",
                "B": "Add the node name as a network tag when creating Compute Engine instances in order to host each workload on the correct node.",
                "C": "Use node affinity labels based on the node group name when creating Compute Engine instances in order to host each workload on the correct node group.",
                "D": "Use node affinity labels based on the node name when creating Compute Engine instances in order to host each workload on the correct node."
            },
            "keyPoints": [
                {
                    "title": "Node Affinity and Workload Separation",
                    "keywords": [
                        "sole-tenant nodes",
                        "Compute Engine",
                        "node affinity"
                    ],
                    "explanation": "Using node affinity labels ensures that each workload is deployed on the correct dedicated host, complying with policies for physical separation and client isolation."
                },
                {
                    "title": "Best Practices for Sensitive Data",
                    "keywords": [
                        "medical data",
                        "data separation",
                        "security"
                    ],
                    "explanation": "By assigning workloads to specific nodes through node affinity labels, you maintain strict control over where sensitive data is processed, adhering to regulatory requirements."
                }
            ],
            "answer": "D",
            "most_voted": "D",
            "page": 43,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60495-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Optimizing Compute Engine for Third-Party Applications",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You are migrating third-party applications from optimized on-premises virtual machines to Google Cloud. You are unsure about the optimum CPU and memory options. The applications have a consistent usage pattern across multiple weeks. You want to optimize resource usage for the lowest cost. What should you do?",
            "type": "option",
            "choices": {
                "A": "Create an instance template with the smallest available machine type, and use an image of the third-party application taken from a current on-premises virtual machine. Create a managed instance group that uses average CPU utilization to autoscale the number of instances in the group. Modify the average CPU utilization threshold to optimize the number of instances running.",
                "B": "Create an App Engine flexible environment, and deploy the third-party application using a Dockerfile and a custom runtime. Set CPU and memory options similar to your application's current on-premises virtual machine in the app.yaml file.",
                "C": "Create multiple Compute Engine instances with varying CPU and memory options. Install the Cloud Monitoring agent, and deploy the third-party application on each of them. Run a load test with high traffic levels on the application, and use the results to determine the optimal settings.",
                "D": "Create a Compute Engine instance with CPU and memory options similar to your application's current on-premises virtual machine. Install the Cloud Monitoring agent, and deploy the third-party application. Run a load test with normal traffic levels on the application, and follow the Rightsizing Recommendations in the Cloud Console."
            },
            "keyPoints": [
                {
                    "title": "Resource Optimization",
                    "keywords": [
                        "Compute Engine",
                        "resource optimization",
                        "rightsizing"
                    ],
                    "explanation": "Running a load test on a Compute Engine instance and following the Rightsizing Recommendations ensures that you find the optimal balance of CPU and memory, reducing costs while maintaining performance."
                },
                {
                    "title": "Cost-Effective Migration",
                    "keywords": [
                        "third-party applications",
                        "cloud migration",
                        "cost optimization"
                    ],
                    "explanation": "By analyzing actual usage data through load tests and rightsizing, you avoid over-provisioning resources, ensuring a cost-effective migration to Google Cloud."
                }
            ],
            "answer": "D",
            "most_voted": "D",
            "page": 44,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60494-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Data Transfer for Machine Learning",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "For this question, refer to the TerramEarth case study. TerramEarth has about 1 petabyte (PB) of vehicle testing data in a private data center. You want to move the data to Cloud Storage for your machine learning team. Currently, a 1-Gbps interconnect link is available for you. The machine learning team wants to start using the data in a month. What should you do?",
            "type": "option",
            "choices": {
                "A": "Request Transfer Appliances from Google Cloud, export the data to appliances, and return the appliances to Google Cloud.",
                "B": "Configure the Storage Transfer service from Google Cloud to send the data from your data center to Cloud Storage.",
                "C": "Make sure there are no other users consuming the 1Gbps link, and use multi-thread transfer to upload the data to Cloud Storage.",
                "D": "Export files to an encrypted USB device, send the device to Google Cloud, and request an import of the data to Cloud Storage."
            },
            "keyPoints": [
                {
                    "title": "Large Data Transfer Solutions",
                    "keywords": [
                        "Transfer Appliance",
                        "data migration",
                        "Cloud Storage"
                    ],
                    "explanation": "Using Transfer Appliances is an effective solution for transferring large amounts of data (like 1 PB) within a short timeframe, ensuring that the machine learning team has access to the data as soon as possible."
                },
                {
                    "title": "Minimizing Impact on Network",
                    "keywords": [
                        "network optimization",
                        "data transfer",
                        "Google Cloud"
                    ],
                    "explanation": "By using Transfer Appliances, you avoid overloading the network, allowing other operations to continue without disruption while the data is securely transferred."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 45,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60483-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Private GKE Cluster with Internet Access",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your team needs to create a Google Kubernetes Engine (GKE) cluster to host a newly built application that requires access to third-party services on the internet. Your company does not allow any Compute Engine instance to have a public IP address on Google Cloud. You need to create a deployment strategy that adheres to these guidelines. What should you do?",
            "type": "option",
            "choices": {
                "A": "Configure the GKE cluster as a private cluster, and configure Cloud NAT Gateway for the cluster subnet.",
                "B": "Configure the GKE cluster as a private cluster. Configure Private Google Access on the Virtual Private Cloud (VPC).",
                "C": "Configure the GKE cluster as a route-based cluster. Configure Private Google Access on the Virtual Private Cloud (VPC).",
                "D": "Create a Compute Engine instance, and install a NAT Proxy on the instance. Configure all workloads on GKE to pass through this proxy to access third-party services on the Internet."
            },
            "keyPoints": [
                {
                    "title": "Internet Access for Private Clusters",
                    "keywords": [
                        "GKE",
                        "private cluster",
                        "Cloud NAT"
                    ],
                    "explanation": "Configuring a private GKE cluster with a Cloud NAT gateway ensures that the cluster can access the internet without exposing any of the instances to a public IP, maintaining security while enabling required functionality."
                },
                {
                    "title": "Adhering to Security Guidelines",
                    "keywords": [
                        "security",
                        "private IP",
                        "Google Cloud"
                    ],
                    "explanation": "By adhering to the security guidelines and using Cloud NAT, you ensure that your deployment strategy maintains the necessary security posture while providing access to external services."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 46,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60441-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Rotating Encryption Keys in Cloud Storage",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your organization has stored sensitive data in a Cloud Storage bucket. For regulatory reasons, your company must be able to rotate the encryption key used to encrypt the data in the bucket. The data will be processed in Dataproc. You want to follow Google-recommended practices for security. What should you do?",
            "type": "option",
            "choices": {
                "A": "Create a key with Cloud Key Management Service (KMS). Encrypt the data using the encrypt method of Cloud KMS.",
                "B": "Create a key with Cloud Key Management Service (KMS). Set the encryption key on the bucket to the Cloud KMS key.",
                "C": "Generate a GPG key pair. Encrypt the data using the GPG key. Upload the encrypted data to the bucket.",
                "D": "Generate an AES-256 encryption key. Encrypt the data in the bucket using the customer-supplied encryption keys feature."
            },
            "keyPoints": [
                {
                    "title": "Managed Key Rotation",
                    "keywords": [
                        "Cloud KMS",
                        "key rotation",
                        "Cloud Storage"
                    ],
                    "explanation": "Using Cloud KMS to manage and rotate encryption keys allows you to securely handle sensitive data and meet regulatory requirements while following best practices in Google Cloud."
                },
                {
                    "title": "Security and Compliance",
                    "keywords": [
                        "encryption",
                        "Cloud Storage",
                        "compliance"
                    ],
                    "explanation": "Setting the Cloud KMS key as the encryption key for the bucket ensures that your organization complies with security regulations and that data is encrypted with a key that can be rotated as needed."
                }
            ],
            "answer": "B",
            "most_voted": "B",
            "page": 47,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60440-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "External Key Management for BigQuery",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You are designing a Data Warehouse on Google Cloud and want to store sensitive data in BigQuery. Your company requires you to generate the encryption keys outside of Google Cloud. You need to implement a solution. What should you do?",
            "type": "option",
            "choices": {
                "A": "Generate a new key in Cloud Key Management Service (Cloud KMS). Store all data in Cloud Storage using the customer-managed key option and select the created key. Set up a Dataflow pipeline to decrypt the data and to store it in a new BigQuery dataset.",
                "B": "Generate a new key in Cloud KMS. Create a dataset in BigQuery using the customer-managed key option and select the created key.",
                "C": "Import a key in Cloud KMS. Store all data in Cloud Storage using the customer-managed key option and select the created key. Set up a Dataflow pipeline to decrypt the data and to store it in a new BigQuery dataset.",
                "D": "Import a key in Cloud KMS. Create a dataset in BigQuery using the customer-supplied key option and select the created key."
            },
            "keyPoints": [
                {
                    "title": "External Key Management",
                    "keywords": [
                        "encryption keys",
                        "Cloud KMS",
                        "customer-supplied keys"
                    ],
                    "explanation": "Importing an externally generated key into Cloud KMS and using it to encrypt a BigQuery dataset with customer-supplied keys ensures that your company’s requirement for external key generation is met."
                },
                {
                    "title": "Sensitive Data Security",
                    "keywords": [
                        "BigQuery",
                        "data warehouse",
                        "encryption"
                    ],
                    "explanation": "Using customer-supplied keys with BigQuery allows you to maintain control over the encryption process, ensuring that sensitive data is securely managed in accordance with company policies."
                }
            ],
            "answer": "D",
            "most_voted": "D",
            "page": 48,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60439-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Securing Container Deployments in GKE",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company has an application running as a Deployment in a Google Kubernetes Engine (GKE) cluster. You have separate clusters for development, staging, and production. You have discovered that the team is able to deploy a Docker image to the production cluster without first testing the deployment in development and then staging. You want to allow the team to have autonomy but want to prevent this from happening. You want a Google Cloud solution that can be implemented quickly with minimal effort. What should you do?",
            "type": "option",
            "choices": {
                "A": "Configure a Kubernetes lifecycle hook to prevent the container from starting if it is not approved for usage in the given environment.",
                "B": "Implement a corporate policy to prevent teams from deploying Docker images to an environment unless the Docker image was tested in an earlier environment.",
                "C": "Configure binary authorization policies for the development, staging, and production clusters. Create attestations as part of the continuous integration pipeline.",
                "D": "Create a Kubernetes admissions controller to prevent the container from starting if it is not approved for usage in the given environment."
            },
            "keyPoints": [
                {
                    "title": "Binary Authorization",
                    "keywords": [
                        "GKE",
                        "binary authorization",
                        "CI/CD"
                    ],
                    "explanation": "Configuring binary authorization ensures that only Docker images that have been tested and approved in the correct environments can be deployed to production, enforcing security and reliability without reducing team autonomy."
                },
                {
                    "title": "Streamlining Deployment Processes",
                    "keywords": [
                        "DevOps",
                        "Kubernetes",
                        "container security"
                    ],
                    "explanation": "Binary authorization integrates with your CI/CD pipeline, providing a streamlined way to enforce deployment policies and ensure that production environments only run validated code."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 49,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60438-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Minimizing Operational Overhead for APIs",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "An application development team has come to you for advice. They are planning to write and deploy an HTTP(S) API using Go 1.12. The API will have a very unpredictable workload and must remain reliable during peaks in traffic. They want to minimize operational overhead for this application. Which approach should you recommend?",
            "type": "option",
            "choices": {
                "A": "Develop the application with containers, and deploy to Google Kubernetes Engine.",
                "B": "Develop the application for App Engine standard environment.",
                "C": "Use a Managed Instance Group when deploying to Compute Engine.",
                "D": "Develop the application for App Engine flexible environment, using a custom runtime."
            },
            "keyPoints": [
                {
                    "title": "Operational Efficiency",
                    "keywords": [
                        "App Engine",
                        "HTTP API",
                        "minimal overhead"
                    ],
                    "explanation": "App Engine standard environment is designed to automatically scale with demand and handle unpredictable workloads, minimizing the need for manual operations and management."
                },
                {
                    "title": "Scalability and Reliability",
                    "keywords": [
                        "scalability",
                        "App Engine",
                        "reliability"
                    ],
                    "explanation": "By using App Engine standard environment, the development team can focus on building their API without worrying about scaling or infrastructure, ensuring that the API remains reliable even during traffic spikes."
                }
            ],
            "answer": "B",
            "most_voted": "B",
            "page": 50,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60437-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Connecting App Engine to On-Premises Database",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company has a support ticketing solution that uses App Engine Standard. The project that contains the App Engine application already has a Virtual Private Cloud (VPC) network fully connected to the company's on-premises environment through a Cloud VPN tunnel. You want to enable the App Engine application to communicate with a database that is running in the company's on-premises environment. What should you do?",
            "choices": {
                "A": "Configure private Google access for on-premises hosts only.",
                "B": "Configure private Google access.",
                "C": "Configure private services access.",
                "D": "Configure serverless VPC access."
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "Serverless VPC Access",
                    "keywords": [
                        "App Engine",
                        "VPC",
                        "serverless VPC access"
                    ],
                    "explanation": "Configuring serverless VPC access allows the App Engine application to connect securely to the on-premises database, leveraging the existing VPN tunnel."
                },
                {
                    "title": "Hybrid Connectivity",
                    "keywords": [
                        "hybrid cloud",
                        "Google Cloud",
                        "on-premises"
                    ],
                    "explanation": "By utilizing serverless VPC access, you can maintain seamless communication between Google Cloud services and on-premises resources, ensuring the application functions properly in a hybrid environment."
                }
            ],
            "answer": "D",
            "most_voted": "D",
            "page": 51,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60436-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Hybrid Connectivity for EHR Healthcare",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "For this question, refer to the EHR Healthcare case study. You need to define the technical architecture for hybrid connectivity between EHR's on-premises systems and Google Cloud. You want to follow Google's recommended practices for production-level applications. Considering the EHR Healthcare business and technical requirements, what should you do?",
            "choices": {
                "A": "Configure two Partner Interconnect connections in one metro (City), and make sure the Interconnect connections are placed in different metro zones.",
                "B": "Configure two VPN connections from on-premises to Google Cloud, and make sure the VPN devices on-premises are in separate racks.",
                "C": "Configure Direct Peering between EHR Healthcare and Google Cloud, and make sure you are peering at least two Google locations.",
                "D": "Configure two Dedicated Interconnect connections in one metro (City) and two connections in another metro, and make sure the Interconnect connections are placed in different metro zones."
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "Dedicated Interconnect for High Availability",
                    "keywords": [
                        "Dedicated Interconnect",
                        "high availability",
                        "Google Cloud"
                    ],
                    "explanation": "Using multiple Dedicated Interconnect connections across different metro zones ensures high availability and redundancy, aligning with best practices for production-level applications."
                },
                {
                    "title": "Hybrid Connectivity Solutions",
                    "keywords": [
                        "hybrid cloud",
                        "Google Cloud",
                        "EHR Healthcare"
                    ],
                    "explanation": "Implementing Dedicated Interconnects across multiple metros provides a reliable and resilient connection between on-premises systems and Google Cloud, essential for critical healthcare applications."
                }
            ],
            "answer": "D",
            "most_voted": "D",
            "page": 52,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60435-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Securely Deploying Verified Containers",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "For this question, refer to the EHR Healthcare case study. You need to define the technical architecture for securely deploying workloads to Google Cloud. You also need to ensure that only verified containers are deployed using Google Cloud services. What should you do? (Choose two.)",
            "choices": {
                "A": "Enable Binary Authorization on GKE, and sign containers as part of a CI/CD pipeline.",
                "B": "Configure Jenkins to utilize Kritis to cryptographically sign a container as part of a CI/CD pipeline.",
                "C": "Configure Container Registry to only allow trusted service accounts to create and deploy containers from the registry.",
                "D": "Configure Container Registry to use vulnerability scanning to confirm that there are no vulnerabilities before deploying the workload."
            },
            "type": "multiple_choice",
            "keyPoints": [
                {
                    "title": "Binary Authorization",
                    "keywords": [
                        "GKE",
                        "Binary Authorization",
                        "CI/CD pipeline"
                    ],
                    "explanation": "Enabling Binary Authorization on GKE ensures that only trusted and verified containers are deployed, adding a layer of security to the containerized workloads."
                },
                {
                    "title": "Container Security",
                    "keywords": [
                        "Container Registry",
                        "vulnerability scanning",
                        "secure deployment"
                    ],
                    "explanation": "Using vulnerability scanning in Container Registry helps identify and mitigate potential security issues before containers are deployed, ensuring that only safe and secure containers run in production."
                }
            ],
            "answer": [
                "A",
                "D"
            ],
            "most_voted": "A,D",
            "page": 53,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60423-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Preventing Data Exfiltration in Google Cloud",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company has a Google Cloud project that uses BigQuery for data warehousing. They have a VPN tunnel between the on-premises environment and Google Cloud that is configured with Cloud VPN. The security team wants to avoid data exfiltration by malicious insiders, compromised code, and accidental oversharing. What should they do?",
            "choices": {
                "A": "Configure Private Google Access for on-premises only.",
                "B": "Perform the following tasks: 1. Create a service account. 2. Give the BigQuery JobUser role and Storage Reader role to the service account. 3. Remove all other IAM access from the project.",
                "C": "Configure VPC Service Controls and configure Private Google Access.",
                "D": "Configure Private Google Access."
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "VPC Service Controls",
                    "keywords": [
                        "data exfiltration",
                        "VPC Service Controls",
                        "Google Cloud"
                    ],
                    "explanation": "VPC Service Controls provide an extra layer of security by defining perimeters around your resources, helping to prevent data exfiltration from Google Cloud services like BigQuery."
                },
                {
                    "title": "Secure Access Configurations",
                    "keywords": [
                        "Private Google Access",
                        "Cloud VPN",
                        "security"
                    ],
                    "explanation": "Combining VPC Service Controls with Private Google Access allows secure communication within the defined perimeter while preventing unauthorized data transfers."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 54,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60416-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Verifying Data Integrity After Upload",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company is planning to upload several important files to Cloud Storage. After the upload is completed, they want to verify that the uploaded content is identical to what they have on-premises. You want to minimize the cost and effort of performing this check. What should you do?",
            "choices": {
                "A": "1. Use Linux shasum to compute a digest of files you want to upload. 2. Use gsutil -m to upload all the files to Cloud Storage. 3. Use gsutil cp to download the uploaded files. 4. Use Linux shasum to compute a digest of the downloaded files. 5. Compare the hashes.",
                "B": "1. Use gsutil -m to upload the files to Cloud Storage. 2. Develop a custom Java application that computes CRC32C hashes. 3. Use gsutil ls -L gs://[YOUR_BUCKET_NAME] to collect CRC32C hashes of the uploaded files. 4. Compare the hashes.",
                "C": "1. Use gsutil -m to upload all the files to Cloud Storage. 2. Use gsutil cp to download the uploaded files. 3. Use Linux diff to compare the content of the files.",
                "D": "1. Use gsutil -m to upload the files to Cloud Storage. 2. Use gsutil hash -c FILE_NAME to generate CRC32C hashes of all on-premises files. 3. Use gsutil ls -L gs://[YOUR_BUCKET_NAME] to collect CRC32C hashes of the uploaded files. 4. Compare the hashes."
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "CRC32C Hash Verification",
                    "keywords": [
                        "data integrity",
                        "gsutil",
                        "CRC32C"
                    ],
                    "explanation": "Using CRC32C hashes is an efficient way to verify that the files uploaded to Cloud Storage are identical to the files stored on-premises, minimizing the effort and cost involved in data verification."
                },
                {
                    "title": "Efficient Data Verification",
                    "keywords": [
                        "gsutil",
                        "data verification",
                        "Cloud Storage"
                    ],
                    "explanation": "By comparing CRC32C hashes before and after the upload, you can ensure data integrity without the need for downloading and rechecking the files, making the process more efficient."
                }
            ],
            "answer": "D",
            "most_voted": "D",
            "page": 55,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60415-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Securing GKE Cluster for EHR Healthcare",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "For this question, refer to the EHR Healthcare case study. You are responsible for designing the Google Cloud network architecture for Google Kubernetes Engine. You want to follow Google best practices. Considering the EHR Healthcare business and technical requirements, what should you do to reduce the attack surface?",
            "choices": {
                "A": "Use a private cluster with a private endpoint with master authorized networks configured.",
                "B": "Use a public cluster with firewall rules and Virtual Private Cloud (VPC) routes.",
                "C": "Use a private cluster with a public endpoint with master authorized networks configured.",
                "D": "Use a public cluster with master authorized networks enabled and firewall rules."
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "Private GKE Cluster",
                    "keywords": [
                        "GKE",
                        "private cluster",
                        "security"
                    ],
                    "explanation": "A private GKE cluster with a private endpoint and master authorized networks significantly reduces the attack surface by limiting access to the Kubernetes master nodes."
                },
                {
                    "title": "Reducing Attack Surface",
                    "keywords": [
                        "network security",
                        "Kubernetes",
                        "EHR Healthcare"
                    ],
                    "explanation": "Using private clusters and authorized networks helps in minimizing potential attack vectors, ensuring the security of sensitive healthcare data."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 56,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60409-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Preventing External IP Configuration",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "For this question, refer to the EHR Healthcare case study. In the past, configuration errors put public IP addresses on backend servers that should not have been accessible from the Internet. You need to ensure that no one can put external IP addresses on backend Compute Engine instances and that external IP addresses can only be configured on frontend Compute Engine instances. What should you do?",
            "choices": {
                "A": "Create an Organizational Policy with a constraint to allow external IP addresses only on the frontend Compute Engine instances.",
                "B": "Revoke the compute.networkAdmin role from all users in the project with front end instances.",
                "C": "Create an Identity and Access Management (IAM) policy that maps the IT staff to the compute.networkAdmin role for the organization.",
                "D": "Create a custom Identity and Access Management (IAM) role named GCE_FRONTEND with the compute.addresses.create permission."
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "Organizational Policies for IP Configuration",
                    "keywords": [
                        "Organizational Policy",
                        "external IP",
                        "Google Cloud"
                    ],
                    "explanation": "Creating an Organizational Policy to restrict external IP addresses ensures that only frontend instances can have public IPs, preventing accidental exposure of backend instances."
                },
                {
                    "title": "Enforcing Security Policies",
                    "keywords": [
                        "IAM",
                        "security",
                        "Google Cloud"
                    ],
                    "explanation": "Using IAM policies and Organizational Policies together provides a robust framework to enforce security controls across the organization, reducing the risk of misconfigurations."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 57,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60407-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Improving Pub/Sub Publishing Latency",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "For this question, refer to the EHR Healthcare case study. You are a developer on the EHR customer portal team. Your team recently migrated the customer portal application to Google Cloud. The load has increased on the application servers, and now the application is logging many timeout errors. You recently incorporated Pub/Sub into the application architecture, and the application is not logging any Pub/Sub publishing errors. You want to improve publishing latency. What should you do?",
            "choices": {
                "A": "Increase the Pub/Sub Total Timeout retry value.",
                "B": "Move from a Pub/Sub subscriber pull model to a push model.",
                "C": "Turn off Pub/Sub message batching.",
                "D": "Create a backup Pub/Sub message queue."
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "Pub/Sub Message Batching",
                    "keywords": [
                        "Pub/Sub",
                        "publishing latency",
                        "Google Cloud"
                    ],
                    "explanation": "Turning off message batching can reduce the time it takes to publish messages by sending them individually, improving overall latency."
                },
                {
                    "title": "Optimizing Pub/Sub Performance",
                    "keywords": [
                        "latency",
                        "Google Cloud",
                        "Pub/Sub"
                    ],
                    "explanation": "Optimizing Pub/Sub settings, such as disabling message batching, can help in meeting performance requirements when high throughput and low latency are critical."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 58,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60405-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Upgrading EHR Connection for Business-Critical Needs",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You need to upgrade the EHR connection to comply with their requirements. The new connection design must support business-critical needs and meet the same network and security policy requirements. What should you do?",
            "choices": {
                "A": "Add a new Dedicated Interconnect connection.",
                "B": "Upgrade the bandwidth on the Dedicated Interconnect connection to 100 G.",
                "C": "Add three new Cloud VPN connections.",
                "D": "Add a new Carrier Peering connection."
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "Dedicated Interconnect for Critical Applications",
                    "keywords": [
                        "Dedicated Interconnect",
                        "network upgrade",
                        "EHR"
                    ],
                    "explanation": "Adding a new Dedicated Interconnect connection ensures redundancy and capacity for business-critical applications, maintaining compliance with network and security policies."
                },
                {
                    "title": "Network Redundancy and Reliability",
                    "keywords": [
                        "network design",
                        "high availability",
                        "Google Cloud"
                    ],
                    "explanation": "Implementing additional Dedicated Interconnects provides the necessary redundancy and bandwidth to support the increased demands of critical healthcare applications."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 59,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60403-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Scaling Kubernetes for I/O-Intensive Processes",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company has a Kubernetes application that pulls messages from Pub/Sub and stores them in Filestore. Because the application is simple, it was deployed as a single pod. The infrastructure team has analyzed Pub/Sub metrics and discovered that the application cannot process the messages in real time. Most of them wait for minutes before being processed. You need to scale the elaboration process that is I/O-intensive. What should you do?",
            "choices": {
                "A": "Use kubectl autoscale deployment APP_NAME --max 6 --min 2 --cpu-percent 50 to configure Kubernetes autoscaling deployment.",
                "B": "Configure a Kubernetes autoscaling deployment based on the subscription/push_request_latencies metric.",
                "C": "Use the --enable-autoscaling flag when you create the Kubernetes cluster.",
                "D": "Configure a Kubernetes autoscaling deployment based on the subscription/num_undelivered_messages metric."
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "Autoscaling Based on Message Backlog",
                    "keywords": [
                        "Kubernetes",
                        "autoscaling",
                        "Pub/Sub"
                    ],
                    "explanation": "Scaling the application based on the number of undelivered messages allows the system to handle increased loads more effectively by allocating additional resources as needed."
                },
                {
                    "title": "Optimizing I/O-Intensive Processes",
                    "keywords": [
                        "I/O-intensive",
                        "scaling",
                        "Kubernetes"
                    ],
                    "explanation": "By configuring autoscaling based on the backlog of messages, you can ensure that the application scales dynamically to meet processing demands, improving performance and reducing latency."
                }
            ],
            "answer": "D",
            "most_voted": "D",
            "page": 60,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60396-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Google Cloud Compliance for EHR",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "For this question, refer to the EHR Healthcare case study. You are responsible for ensuring that EHR's use of Google Cloud will pass an upcoming privacy compliance audit. What should you do? (Choose two.)",
            "choices": {
                "A": "Verify EHR's product usage against the list of compliant products on the Google Cloud compliance page.",
                "B": "Advise EHR to execute a Business Associate Agreement (BAA) with Google Cloud.",
                "C": "Use Firebase Authentication for EHR's user facing applications.",
                "D": "Implement Prometheus to detect and prevent security breaches on EHR's web-based applications.",
                "E": "Use GKE private clusters for all Kubernetes workloads."
            },
            "type": "multiple_choice",
            "keyPoints": [
                {
                    "title": "Google Cloud Compliance",
                    "keywords": [
                        "compliance",
                        "Google Cloud",
                        "privacy audit"
                    ],
                    "explanation": "Ensuring that EHR's product usage aligns with the compliant products listed on the Google Cloud compliance page is crucial for passing a privacy compliance audit."
                },
                {
                    "title": "Business Associate Agreement (BAA)",
                    "keywords": [
                        "BAA",
                        "Google Cloud",
                        "privacy compliance"
                    ],
                    "explanation": "Executing a Business Associate Agreement (BAA) with Google Cloud is necessary to ensure that Google Cloud services are used in compliance with HIPAA regulations."
                }
            ],
            "answer": [
                "A",
                "B"
            ],
            "most_voted": "AB",
            "page": 61,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/60388-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Microservices Communication in GKE",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You are developing an application using different microservices that should remain internal to the cluster. You want to be able to configure each microservice with a specific number of replicas. You also want to be able to address a specific microservice from any other microservice in a uniform way, regardless of the number of replicas the microservice scales to. You need to implement this solution on Google Kubernetes Engine. What should you do?",
            "choices": {
                "A": "Deploy each microservice as a Deployment. Expose the Deployment in the cluster using a Service, and use the Service DNS name to address it from other microservices within the cluster.",
                "B": "Deploy each microservice as a Deployment. Expose the Deployment in the cluster using an Ingress, and use the Ingress IP address to address the Deployment from other microservices within the cluster.",
                "C": "Deploy each microservice as a Pod. Expose the Pod in the cluster using a Service, and use the Service DNS name to address the microservice from other microservices within the cluster.",
                "D": "Deploy each microservice as a Pod. Expose the Pod in the cluster using an Ingress, and use the Ingress IP address name to address the Pod from other microservices within the cluster."
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "Service DNS in GKE",
                    "keywords": [
                        "Kubernetes",
                        "microservices",
                        "Service DNS"
                    ],
                    "explanation": "Deploying microservices as a Deployment and using a Service to expose them within the cluster allows you to scale the replicas and access the service uniformly via DNS."
                },
                {
                    "title": "Internal Microservices Communication",
                    "keywords": [
                        "GKE",
                        "internal communication",
                        "microservices"
                    ],
                    "explanation": "Using Kubernetes Services for internal microservices communication ensures that the architecture remains scalable and manageable while maintaining uniform addressing."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 62,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/57424-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Managing Roles for Compute and Network Admins",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company has a networking team and a development team. The development team runs applications on Compute Engine instances that contain sensitive data. The development team requires administrative permissions for Compute Engine. Your company requires all network resources to be managed by the networking team. The development team does not want the networking team to have access to the sensitive data on the instances. What should you do?",
            "choices": {
                "A": "1. Create a project with a standalone VPC and assign the Network Admin role to the networking team. 2. Create a second project with a standalone VPC and assign the Compute Admin role to the development team. 3. Use Cloud VPN to join the two VPCs.",
                "B": "1. Create a project with a standalone Virtual Private Cloud (VPC), assign the Network Admin role to the networking team, and assign the Compute Admin role to the development team.",
                "C": "1. Create a project with a Shared VPC and assign the Network Admin role to the networking team. 2. Create a second project without a VPC, configure it as a Shared VPC service project, and assign the Compute Admin role to the development team.",
                "D": "1. Create a project with a standalone VPC and assign the Network Admin role to the networking team. 2. Create a second project with a standalone VPC and assign the Compute Admin role to the development team. 3. Use VPC Peering to join the two VPCs."
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "Shared VPC for Role Separation",
                    "keywords": [
                        "Shared VPC",
                        "Compute Admin",
                        "Network Admin"
                    ],
                    "explanation": "Using a Shared VPC allows the networking team to manage network resources while the development team can manage Compute Engine instances, ensuring that sensitive data remains protected."
                },
                {
                    "title": "Role-Based Access Control",
                    "keywords": [
                        "RBAC",
                        "IAM roles",
                        "Google Cloud"
                    ],
                    "explanation": "Implementing separate projects with Shared VPC enables clear role-based access control, preventing unauthorized access to sensitive data."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 63,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/57301-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Managing Debian Linux Updates on Compute Engine",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You need to deploy an application on Google Cloud that must run on a Debian Linux environment. The application requires extensive configuration in order to operate correctly. You want to ensure that you can install Debian distribution updates with minimal manual intervention whenever they become available. What should you do?",
            "choices": {
                "A": "Create a Compute Engine instance template using the most recent Debian image. Create an instance from this template, and install and configure the application as part of the startup script. Repeat this process whenever a new Google-managed Debian image becomes available.",
                "B": "Create a Debian-based Compute Engine instance, install and configure the application, and use OS patch management to install available updates.",
                "C": "Create an instance with the latest available Debian image. Connect to the instance via SSH, and install and configure the application on the instance. Repeat this process whenever a new Google-managed Debian image becomes available.",
                "D": "Create a Docker container with Debian as the base image. Install and configure the application as part of the Docker image creation process. Host the container on Google Kubernetes Engine and restart the container whenever a new update is available."
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "OS Patch Management",
                    "keywords": [
                        "Debian Linux",
                        "OS patch management",
                        "Google Cloud"
                    ],
                    "explanation": "Using OS patch management on Compute Engine ensures that the Debian environment receives updates automatically, reducing the need for manual intervention."
                },
                {
                    "title": "Debian Updates with Minimal Effort",
                    "keywords": [
                        "Compute Engine",
                        "Debian updates",
                        "automation"
                    ],
                    "explanation": "Automating updates through OS patch management allows the application to remain secure and up-to-date with minimal manual intervention."
                }
            ],
            "answer": "B",
            "most_voted": "B",
            "page": 64,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/57270-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Optimizing Cloud Storage Lifecycle for Cost",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "For this question, refer to the TerramEarth case study. TerramEarth has decided to store data files in Cloud Storage. You need to configure Cloud Storage lifecycle rule to store 1 year of data and minimize file storage cost. Which two actions should you take?",
            "choices": {
                "A": "Create a Cloud Storage lifecycle rule with Age: 30, Storage Class: Standard, and Action: Set to Coldline, and create a second GCS life-cycle rule with Age: 365, Storage Class: Coldline, and Action: Delete.",
                "B": "Create a Cloud Storage lifecycle rule with Age: 30, Storage Class: Coldline, and Action: Set to Nearline, and create a second GCS life-cycle rule with Age: 91, Storage Class: Coldline, and Action: Set to Nearline.",
                "C": "Create a Cloud Storage lifecycle rule with Age: 90, Storage Class: Standard, and Action: Set to Nearline, and create a second GCS life-cycle rule with Age: 91, Storage Class: Nearline, and Action: Set to Coldline.",
                "D": "Create a Cloud Storage lifecycle rule with Age: 30, Storage Class: Standard, and Action: Set to Coldline, and create a second GCS life-cycle rule with Age: 365, Storage Class: Nearline, and Action: Delete."
            },
            "type": "multiple_choice",
            "keyPoints": [
                {
                    "title": "Cloud Storage Lifecycle Management",
                    "keywords": [
                        "Cloud Storage",
                        "lifecycle management",
                        "cost optimization"
                    ],
                    "explanation": "Implementing lifecycle rules that transition data from Standard to Coldline storage and eventually delete the data ensures cost-effective storage over time."
                },
                {
                    "title": "Minimizing Storage Costs",
                    "keywords": [
                        "Coldline",
                        "Nearline",
                        "data retention"
                    ],
                    "explanation": "Moving data to colder storage tiers after 30 days and deleting it after one year minimizes storage costs while retaining data as needed."
                }
            ],
            "answer": [
                "A"
            ],
            "most_voted": "A",
            "page": 65,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/57128-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Google Cloud Logging and Monitoring",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company sends all Google Cloud logs to Cloud Logging. Your security team wants to monitor the logs. You want to ensure that the security team can react quickly if an anomaly such as an unwanted firewall change or server breach is detected. You want to follow Google-recommended practices. What should you do?",
            "choices": {
                "A": "Schedule a cron job with Cloud Scheduler. The scheduled job queries the logs every minute for the relevant events.",
                "B": "Export logs to BigQuery, and trigger a query in BigQuery to process the log data for the relevant events.",
                "C": "Export logs to a Pub/Sub topic, and trigger Cloud Function with the relevant log events.",
                "D": "Export logs to a Cloud Storage bucket, and trigger Cloud Run with the relevant log events."
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "Real-Time Monitoring with Cloud Pub/Sub",
                    "keywords": [
                        "Cloud Logging",
                        "Pub/Sub",
                        "Cloud Functions",
                        "real-time monitoring"
                    ],
                    "explanation": "Exporting logs to Pub/Sub and using Cloud Functions for real-time event processing ensures that the security team can react quickly to anomalies."
                },
                {
                    "title": "Google-Recommended Practices",
                    "keywords": [
                        "best practices",
                        "Google Cloud",
                        "security monitoring"
                    ],
                    "explanation": "This approach leverages Google-recommended practices for real-time monitoring and response to security threats in a cloud environment."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 66,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/57043-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Identifying Microservices Bottlenecks in Anthos",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company has an application deployed on Anthos clusters (formerly Anthos GKE) that is running multiple microservices. The cluster has both Anthos ServiceMesh and Anthos Config Management configured. End users inform you that the application is responding very slowly. You want to identify the microservice that is causing the delay. What should you do?",
            "choices": {
                "A": "Use the Service Mesh visualization in the Cloud Console to inspect the telemetry between the microservices.",
                "B": "Use Anthos Config Management to create a ClusterSelector selecting the relevant cluster. On the Google Cloud Console page for Google Kubernetes Engine, view the Workloads and filter on the cluster. Inspect the configurations of the filtered workloads.",
                "C": "Use Anthos Config Management to create a namespaceSelector selecting the relevant cluster namespace. On the Google Cloud Console page for Google Kubernetes Engine, visit the workloads and filter on the namespace. Inspect the configurations of the filtered workloads.",
                "D": "Reinstall istio using the default istio profile in order to collect request latency. Evaluate the telemetry between the microservices in the Cloud Console."
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "Service Mesh Visualization",
                    "keywords": [
                        "Anthos",
                        "Service Mesh",
                        "microservices",
                        "latency"
                    ],
                    "explanation": "Service Mesh visualization provides an overview of telemetry between microservices, making it easier to identify the source of delays."
                },
                {
                    "title": "Cloud Console Tools",
                    "keywords": [
                        "Google Cloud Console",
                        "telemetry",
                        "performance monitoring"
                    ],
                    "explanation": "Utilizing the visualization tools in the Cloud Console helps in quickly diagnosing performance issues in a microservices architecture."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 67,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/57009-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Data Retention and Security in Cloud Storage",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You are working at a financial institution that stores mortgage loan approval documents on Cloud Storage. Any change to these approval documents must be uploaded as a separate approval file, so you want to ensure that these documents cannot be deleted or overwritten for the next 5 years. What should you do?",
            "choices": {
                "A": "Create a retention policy on the bucket for the duration of 5 years. Create a lock on the retention policy.",
                "B": "Create the bucket with uniform bucket-level access, and grant a service account the role of Object Writer. Use the service account to upload new files.",
                "C": "Use a customer-managed key for the encryption of the bucket. Rotate the key after 5 years.",
                "D": "Create the bucket with fine-grained access control, and grant a service account the role of Object Writer. Use the service account to upload new files."
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "Retention Policy",
                    "keywords": [
                        "Cloud Storage",
                        "retention policy",
                        "data protection"
                    ],
                    "explanation": "A retention policy ensures that documents cannot be deleted or overwritten for a specified duration, meeting compliance and security requirements."
                },
                {
                    "title": "Compliance and Security",
                    "keywords": [
                        "financial data",
                        "compliance",
                        "data retention"
                    ],
                    "explanation": "Implementing a retention policy and locking it secures sensitive financial documents against accidental or malicious deletion."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 68,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/57007-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Firestore Access Management",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You are implementing Firestore for Mountkirk Games. Mountkirk Games wants to give a new game programmatic access to a legacy game's Firestore database. Access should be as restricted as possible. What should you do?",
            "choices": {
                "A": "Create a service account (SA) in the legacy game's Google Cloud project, add a second SA in the new game's IAM page, and then give the Organization Admin role to both SAs.",
                "B": "Create a service account (SA) in the legacy game's Google Cloud project, give the SA the Organization Admin role, and then give it the Firebase Admin role in both projects.",
                "C": "Create a service account (SA) in the legacy game's Google Cloud project, add this SA in the new game's IAM page, and then give it the Firebase Admin role in both projects.",
                "D": "Create a service account (SA) in the legacy game's Google Cloud project, give it the Firebase Admin role, and then migrate the new game to the legacy game's project."
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "Service Account Management",
                    "keywords": [
                        "Firestore",
                        "service account",
                        "access control"
                    ],
                    "explanation": "Assigning a service account with the Firebase Admin role ensures that the new game has the necessary access while restricting broader permissions."
                },
                {
                    "title": "Access Restriction",
                    "keywords": [
                        "access control",
                        "Firebase Admin",
                        "IAM roles"
                    ],
                    "explanation": "Minimizing access by using specific IAM roles and service accounts aligns with best practices for securing sensitive data in Firestore."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 69,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/56978-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Optimizing Batch File Transfers",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You need to optimize batch file transfers into Cloud Storage for Mountkirk Games' new Google Cloud solution. The batch files contain game statistics that need to be staged in Cloud Storage and be processed by an extract transform load (ETL) tool. What should you do?",
            "choices": {
                "A": "Use gsutil to batch move files in sequence.",
                "B": "Use gsutil to batch copy the files in parallel.",
                "C": "Use gsutil to extract the files as the first part of ETL.",
                "D": "Use gsutil to load the files as the last part of ETL."
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "Parallel File Transfer",
                    "keywords": [
                        "gsutil",
                        "parallel copy",
                        "Cloud Storage"
                    ],
                    "explanation": "Using gsutil to copy files in parallel optimizes transfer speed, making it suitable for processing large batches of data efficiently."
                },
                {
                    "title": "Optimizing ETL Processes",
                    "keywords": [
                        "ETL",
                        "Cloud Storage",
                        "gsutil"
                    ],
                    "explanation": "Efficiently transferring large datasets to Cloud Storage allows for timely processing in ETL workflows, which is critical for timely data analysis."
                }
            ],
            "answer": "B",
            "most_voted": "B",
            "page": 70,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/56977-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Microservices Architecture and Managed Services",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company has developed a monolithic, 3-tier application to allow external users to upload and share files. The solution cannot be easily enhanced and lacks reliability. The development team would like to re-architect the application to adopt microservices and a fully managed service approach, but they need to convince their leadership that the effort is worthwhile. Which advantage(s) should they highlight to leadership?",
            "choices": {
                "A": "The new approach will be significantly less costly, make it easier to manage the underlying infrastructure, and automatically manage the CI/CD pipelines.",
                "B": "The monolithic solution can be converted to a container with Docker. The generated container can then be deployed into a Kubernetes cluster.",
                "C": "The new approach will make it easier to decouple infrastructure from application, develop and release new features, manage the underlying infrastructure, manage CI/CD pipelines and perform A/B testing, and scale the solution if necessary.",
                "D": "The process can be automated with Migrate for Compute Engine."
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "Advantages of Microservices",
                    "keywords": [
                        "microservices",
                        "managed services",
                        "scalability"
                    ],
                    "explanation": "Re-architecting to microservices enables easier scalability, better infrastructure management, more efficient CI/CD pipelines, and streamlined feature development and release processes."
                },
                {
                    "title": "Leadership Buy-In",
                    "keywords": [
                        "cost-effectiveness",
                        "infrastructure management",
                        "CI/CD pipelines"
                    ],
                    "explanation": "Emphasizing the reduced cost, improved manageability, and enhanced development processes can help gain leadership support for adopting microservices and managed services."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 71,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/56976-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Managing Permissions in Google Cloud",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company is using Google Cloud. You have two folders under the Organization: Finance and Shopping. The members of the development team are in a Google Group. The development team group has been assigned the Project Owner role on the Organization. You want to prevent the development team from creating resources in projects in the Finance folder. What should you do?",
            "choices": {
                "A": "Assign the development team group the Project Viewer role on the Finance folder, and assign the development team group the Project Owner role on the Shopping folder.",
                "B": "Assign the development team group only the Project Viewer role on the Finance folder.",
                "C": "Assign the development team group the Project Owner role on the Shopping folder, and remove the development team group Project Owner role from the Organization.",
                "D": "Assign the development team group only the Project Owner role on the Shopping folder."
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "Role Assignment",
                    "keywords": [
                        "Project Owner",
                        "Project Viewer",
                        "role management"
                    ],
                    "explanation": "Removing the Project Owner role from the Organization and assigning it specifically to the Shopping folder prevents the development team from creating resources in the Finance folder."
                },
                {
                    "title": "Security and Access Control",
                    "keywords": [
                        "access control",
                        "folder permissions",
                        "Google Groups"
                    ],
                    "explanation": "Limiting the roles at the folder level enhances security by ensuring that only necessary permissions are granted for specific projects."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 72,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/56975-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Automating Security Testing with Cloud Functions",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "For this question, refer to the Helicopter Racing League (HRL) case study. The HRL development team releases a new version of their predictive capability application every Tuesday evening at 3 a.m. UTC to a repository. The security team at HRL has developed an in-house penetration test Cloud Function called Airwolf. The security team wants to run Airwolf against the predictive capability application as soon as it is released every Tuesday. You need to set up Airwolf to run at the recurring weekly cadence. What should you do?",
            "choices": {
                "A": "Set up Cloud Tasks and a Cloud Storage bucket that triggers a Cloud Function.",
                "B": "Set up a Cloud Logging sink and a Cloud Storage bucket that triggers a Cloud Function.",
                "C": "Configure the deployment job to notify a Pub/Sub queue that triggers a Cloud Function.",
                "D": "Set up Identity and Access Management (IAM) and Confidential Computing to trigger a Cloud Function."
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "Automation with Cloud Pub/Sub",
                    "keywords": [
                        "Cloud Pub/Sub",
                        "Cloud Function",
                        "automation"
                    ],
                    "explanation": "Using Cloud Pub/Sub to trigger a Cloud Function upon deployment ensures that the Airwolf penetration test is automatically executed at the specified time."
                },
                {
                    "title": "Security Testing Integration",
                    "keywords": [
                        "penetration testing",
                        "Cloud Function",
                        "CI/CD"
                    ],
                    "explanation": "Integrating security testing with CI/CD pipelines allows for continuous validation of new application releases, improving overall application security."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 73,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/56890-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Lift-and-Shift Migration to Compute Engine",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company is planning to perform a lift and shift migration of their Linux RHEL 6.5+ virtual machines. The virtual machines are running in an on-premises VMware environment. You want to migrate them to Compute Engine following Google-recommended practices. What should you do?",
            "choices": {
                "A": "1. Define a migration plan based on the list of the applications and their dependencies. 2. Migrate all virtual machines into Compute Engine individually with Migrate for Compute Engine.",
                "B": "1. Perform an assessment of virtual machines running in the current VMware environment. 2. Create images of all disks. Import disks on Compute Engine. 3. Create standard virtual machines where the boot disks are the ones you have imported.",
                "C": "1. Perform an assessment of virtual machines running in the current VMware environment. 2. Define a migration plan, prepare a Migrate for Compute Engine migration RunBook, and execute the migration.",
                "D": "1. Perform an assessment of virtual machines running in the current VMware environment. 2. Install a third-party agent on all selected virtual machines. 3. Migrate all virtual machines into Compute Engine."
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "Migration Planning",
                    "keywords": [
                        "migration",
                        "Compute Engine",
                        "VMware"
                    ],
                    "explanation": "Carefully planning the migration and creating a migration RunBook ensures a smooth transition from on-premises to Compute Engine, minimizing downtime and errors."
                },
                {
                    "title": "Google-Recommended Practices",
                    "keywords": [
                        "best practices",
                        "Migrate for Compute Engine",
                        "assessment"
                    ],
                    "explanation": "Following Google-recommended practices for lift-and-shift migration ensures that the virtual machines are correctly and efficiently moved to the cloud."
                }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 74,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/56841-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Migrating to App Engine for Operational Efficiency",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company has announced that they will be outsourcing operations functions. You want to allow developers to easily stage new versions of a cloud-based application in the production environment and allow the outsourced operations team to autonomously promote staged versions to production. You want to minimize the operational overhead of the solution. Which Google Cloud product should you migrate to?",
            "choices": {
                "A": "App Engine",
                "B": "GKE On-Prem",
                "C": "Compute Engine",
                "D": "Google Kubernetes Engine"
            },
            "type": "option",
            "keyPoints": [
                {
                    "title": "App Engine for Managed Services",
                    "keywords": [
                        "App Engine",
                        "managed services",
                        "operational efficiency"
                    ],
                    "explanation": "App Engine provides a fully managed platform that allows developers to focus on code while the operations team can manage deployments with minimal overhead."
                },
                {
                    "title": "Outsourcing Operations",
                    "keywords": [
                        "staging",
                        "production",
                        "outsourcing"
                    ],
                    "explanation": "Using App Engine enables smooth handover of operational responsibilities to an outsourced team, reducing complexity and ensuring reliable application management."
                }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 75,
            "URL": [
                "https://www.examtopics.com/discussions/google/view/56840-exam-professional-cloud-architect-topic-1-question-126/"
            ]
        },
        {
            "source": "",
            "topic": "Mobile App Testing on Google Cloud",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your development team has created a mobile game app. You want to test the new mobile app on Android and iOS devices with a variety of configurations. You need to ensure that testing is efficient and cost-effective. What should you do?",
            "choices": {
              "A": "Upload your mobile app to the Firebase Test Lab, and test the mobile app on Android and iOS devices.",
              "B": "Create Android and iOS VMs on Google Cloud, install the mobile app on the VMs, and test the mobile app.",
              "C": "Create Android and iOS containers on Google Kubernetes Engine (GKE), install the mobile app on the containers, and test the mobile app.",
              "D": "Upload your mobile app with different configurations to Firebase Hosting and test each configuration."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "Firebase Test Lab",
                "keywords": ["Firebase Test Lab", "Android", "iOS", "mobile app testing"],
                "explanation": "Firebase Test Lab allows you to test your app on a wide range of real Android and iOS devices, ensuring thorough and cost-effective testing."
              },
              {
                "title": "Cost-Effective Testing",
                "keywords": ["cost-effective", "automated testing", "device configurations"],
                "explanation": "Using Firebase Test Lab is an efficient way to test various configurations of your app without the overhead of managing virtual machines or containers."
              }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 76,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56798-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "Managed Services for High-Scalability Applications",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company is developing a new application that will allow globally distributed users to upload pictures and share them with other selected users. The application will support millions of concurrent users. You want to allow developers to focus on just building code without having to create and maintain the underlying infrastructure. Which service should you use to deploy the application?",
            "choices": {
              "A": "App Engine",
              "B": "Cloud Endpoints",
              "C": "Compute Engine",
              "D": "Google Kubernetes Engine"
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "App Engine for High-Scalability",
                "keywords": ["App Engine", "scalability", "managed services"],
                "explanation": "App Engine is a fully managed platform that automatically scales to support millions of concurrent users, allowing developers to focus on code without worrying about infrastructure."
              },
              {
                "title": "Focus on Development",
                "keywords": ["development focus", "infrastructure management", "automatic scaling"],
                "explanation": "By using App Engine, developers can concentrate on building the application, while the platform handles scaling and infrastructure management."
              }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 77,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56754-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "Secure SSH Access Without Public IP",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You have deployed several instances on Compute Engine. As a security requirement, instances cannot have a public IP address. There is no VPN connection between Google Cloud and your office, and you need to connect via SSH into a specific machine without violating the security requirements. What should you do?",
            "choices": {
              "A": "Configure Cloud NAT on the subnet where the instance is hosted. Create an SSH connection to the Cloud NAT IP address to reach the instance.",
              "B": "Add all instances to an unmanaged instance group. Configure TCP Proxy Load Balancing with the instance group as a backend. Connect to the instance using the TCP Proxy IP.",
              "C": "Configure Identity-Aware Proxy (IAP) for the instance and ensure that you have the role of IAP-secured Tunnel User. Use the gcloud command line tool to ssh into the instance.",
              "D": "Create a bastion host in the network to SSH into the bastion host from your office location. From the bastion host, SSH into the desired instance."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "Identity-Aware Proxy (IAP) for Secure SSH",
                "keywords": ["Identity-Aware Proxy", "SSH", "secure access"],
                "explanation": "Using IAP allows secure SSH access to instances without public IP addresses, adhering to security requirements while providing the necessary access."
              },
              {
                "title": "Security Compliance",
                "keywords": ["security compliance", "no public IP", "IAP-secured Tunnel User"],
                "explanation": "IAP ensures that security policies are followed by preventing direct external access while still enabling secure internal connections."
              }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 78,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56751-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "Service Level Indicators (SLIs) for GKE",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your development teams release new versions of games running on Google Kubernetes Engine (GKE) daily. You want to create service level indicators (SLIs) to evaluate the quality of the new versions from the user's perspective. What should you do?",
            "choices": {
              "A": "Create CPU Utilization and Request Latency as service level indicators.",
              "B": "Create GKE CPU Utilization and Memory Utilization as service level indicators.",
              "C": "Create Request Latency and Error Rate as service level indicators.",
              "D": "Create Server Uptime and Error Rate as service level indicators."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "User-Centric SLIs",
                "keywords": ["SLIs", "Request Latency", "Error Rate", "user experience"],
                "explanation": "Request Latency and Error Rate are crucial indicators that reflect the user's experience with the application, making them essential for evaluating the quality of new versions."
              },
              {
                "title": "Performance Evaluation",
                "keywords": ["GKE", "performance metrics", "quality evaluation"],
                "explanation": "Monitoring these SLIs helps ensure that the application meets performance and reliability standards expected by users."
              }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 79,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56735-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "Network Ingress for Multi-Region Game Deployment",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You need to implement a network ingress for a new game that meets the defined business and technical requirements. Mountkirk Games wants each regional game instance to be located in multiple Google Cloud regions. What should you do?",
            "choices": {
              "A": "Configure a global load balancer connected to a managed instance group running Compute Engine instances.",
              "B": "Configure kubemci with a global load balancer and Google Kubernetes Engine.",
              "C": "Configure a global load balancer with Google Kubernetes Engine.",
              "D": "Configure Ingress for Anthos with a global load balancer and Google Kubernetes Engine."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "Global Load Balancing with Anthos",
                "keywords": ["Ingress for Anthos", "global load balancer", "multi-region deployment"],
                "explanation": "Using Ingress for Anthos with a global load balancer provides robust support for multi-region deployments, ensuring low-latency and high-availability gaming experiences."
              },
              {
                "title": "Meeting Business and Technical Requirements",
                "keywords": ["business requirements", "technical requirements", "Google Cloud regions"],
                "explanation": "This configuration meets the game's business and technical requirements by ensuring that each regional instance is accessible and performs optimally across multiple regions."
              }
            ],
            "answer": "D",
            "most_voted": "D",
            "page": 80,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56734-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "Resource Location Limitation in Google Cloud",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Mountkirk Games wants to limit the physical location of resources to their operating Google Cloud regions. What should you do?",
            "choices": {
              "A": "Configure an organizational policy which constrains where resources can be deployed.",
              "B": "Configure IAM conditions to limit what resources can be configured.",
              "C": "Configure the quotas for resources in the regions not being used to 0.",
              "D": "Configure a custom alert in Cloud Monitoring so you can disable resources as they are created in other regions."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "Organizational Policy for Resource Location",
                "keywords": ["organizational policy", "resource location", "Google Cloud regions"],
                "explanation": "Configuring an organizational policy to constrain where resources can be deployed ensures that resources are only created in specified regions, aligning with company policies."
              },
              {
                "title": "Best Practice",
                "keywords": ["best practice", "resource management", "Google Cloud"],
                "explanation": "Using an organizational policy is the recommended approach to enforce geographical restrictions on where resources can be provisioned, ensuring compliance with company guidelines."
              }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 81,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56732-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "Load Testing and Latency Validation on GKE",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your team is developing a web application that will be deployed on Google Kubernetes Engine (GKE). Your CTO expects a successful launch and you need to ensure your application can handle the expected load of tens of thousands of users. You want to test the current deployment to ensure the latency of your application stays below a certain threshold. What should you do?",
            "choices": {
              "A": "Use a load testing tool to simulate the expected number of concurrent users and total requests to your application, and inspect the results.",
              "B": "Enable autoscaling on the GKE cluster and enable horizontal pod autoscaling on your application deployments. Send curl requests to your application, and validate if the auto scaling works.",
              "C": "Replicate the application over multiple GKE clusters in every Google Cloud region. Configure a global HTTP(S) load balancer to expose the different clusters over a single global IP address.",
              "D": "Use Cloud Debugger in the development environment to understand the latency between the different microservices."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "Load Testing for Scalability",
                "keywords": ["load testing", "scalability", "latency"],
                "explanation": "Load testing is essential for validating that the application can handle the expected user load while maintaining acceptable latency levels."
              },
              {
                "title": "Performance Assurance",
                "keywords": ["performance testing", "concurrent users", "GKE"],
                "explanation": "Simulating real-world usage conditions through load testing ensures that the application performs well under high demand, meeting the expected quality of service."
              }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 82,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56706-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "Automated CI/CD Pipeline for Microservices",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your team will start developing a new application using microservices architecture on Kubernetes Engine. As part of the development lifecycle, any code change that has been pushed to the remote develop branch on your GitHub repository should be built and tested automatically. When the build and test are successful, the relevant microservice will be deployed automatically in the development environment. You want to ensure that all code deployed in the development environment follows this process. What should you do?",
            "choices": {
              "A": "Have each developer install a pre-commit hook on their workstation that tests the code and builds the container when committing on the development branch. After a successful commit, have the developer deploy the newly built container image on the development cluster.",
              "B": "Install a post-commit hook on the remote git repository that tests the code and builds the container when code is pushed to the development branch. After a successful commit, have the developer deploy the newly built container image on the development cluster.",
              "C": "Create a Cloud Build trigger based on the development branch that tests the code, builds the container, and stores it in Container Registry. Create a deployment pipeline that watches for new images and deploys the new image on the development cluster. Ensure only the deployment tool has access to deploy new versions.",
              "D": "Create a Cloud Build trigger based on the development branch to build a new container image and store it in Container Registry. Rely on Vulnerability Scanning to ensure the code tests succeed. As the final step of the Cloud Build process, deploy the new container image on the development cluster. Ensure only Cloud Build has access to deploy new versions."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "Automated CI/CD Pipeline",
                "keywords": ["CI/CD pipeline", "Cloud Build", "microservices", "Kubernetes"],
                "explanation": "Using Cloud Build triggers for automated testing, building, and deployment ensures that the development process is consistent, efficient, and reliable."
              },
              {
                "title": "Deployment Security",
                "keywords": ["deployment security", "access control", "development environment"],
                "explanation": "Ensuring that only the deployment tool can deploy new versions adds a layer of security, preventing unauthorized changes to the environment."
              }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 83,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56702-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "Minimal Downtime MySQL Migration",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You are moving an application that uses MySQL from on-premises to Google Cloud. The application will run on Compute Engine and will use Cloud SQL. You want to cut over to the Compute Engine deployment of the application with minimal downtime and no data loss to your customers. You want to migrate the application with minimal modification. You also need to determine the cutover strategy. What should you do?",
            "choices": {
              "A": "1. Set up Cloud VPN to provide private network connectivity between the Compute Engine application and the on-premises MySQL server. 2. Stop the on-premises application. 3. Create a mysqldump of the on-premises MySQL server. 4. Upload the dump to a Cloud Storage bucket. 5. Import the dump into Cloud SQL. 6. Modify the source code of the application to write queries to both databases and read from its local database. 7. Start the Compute Engine application. 8. Stop the on-premises application.",
              "B": "1. Set up Cloud SQL proxy and MySQL proxy. 2. Create a mysqldump of the on-premises MySQL server. 3. Upload the dump to a Cloud Storage bucket. 4. Import the dump into Cloud SQL. 5. Stop the on-premises application. 6. Start the Compute Engine application.",
              "C": "1. Set up Cloud VPN to provide private network connectivity between the Compute Engine application and the on-premises MySQL server. 2. Stop the on-premises application. 3. Start the Compute Engine application, configured to read and write to the on-premises MySQL server. 4. Create the replication configuration in Cloud SQL. 5. Configure the source database server to accept connections from the Cloud SQL replica. 6. Finalize the Cloud SQL replica configuration. 7. When replication has been completed, stop the Compute Engine application. 8. Promote the Cloud SQL replica to a standalone instance. 9. Restart the Compute Engine application, configured to read and write to the Cloud SQL standalone instance.",
              "D": "1. Stop the on-premises application. 2. Create a mysqldump of the on-premises MySQL server. 3. Upload the dump to a Cloud Storage bucket. 4. Import the dump into Cloud SQL. 5. Start the application on Compute Engine."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "Cutover Strategy",
                "keywords": ["MySQL migration", "Cloud SQL", "minimal downtime", "replication"],
                "explanation": "Setting up replication between the on-premises MySQL server and Cloud SQL ensures that the data remains consistent and that the cutover happens with minimal downtime."
              },
              {
                "title": "Minimal Modification",
                "keywords": ["minimal modification", "Compute Engine", "database migration"],
                "explanation": "This approach minimizes the need for code changes, as the application can continue to use MySQL with minimal modification."
              }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 84,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56692-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "Cost Optimization for Development and Acceptance Environments",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company is running its application workloads on Compute Engine. The applications have been deployed in production, acceptance, and development environments. The production environment is business-critical and is used 24/7, while the acceptance and development environments are only critical during office hours. Your CFO has asked you to optimize these environments to achieve cost savings during idle times. What should you do?",
            "choices": {
              "A": "Create a shell script that uses the gcloud command to change the machine type of the development and acceptance instances to a smaller machine type outside of office hours. Schedule the shell script on one of the production instances to automate the task.",
              "B": "Use Cloud Scheduler to trigger a Cloud Function that will stop the development and acceptance environments after office hours and start them just before office hours.",
              "C": "Deploy the development and acceptance applications on a managed instance group and enable autoscaling.",
              "D": "Use regular Compute Engine instances for the production environment, and use preemptible VMs for the acceptance and development environments."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "Cost Optimization",
                "keywords": ["cost savings", "Cloud Scheduler", "Compute Engine", "idle time"],
                "explanation": "Using Cloud Scheduler to automate the start and stop of non-critical environments outside office hours optimizes costs without affecting production availability."
              },
              {
                "title": "Automation",
                "keywords": ["automation", "Cloud Function", "operational efficiency"],
                "explanation": "Automating the process with Cloud Functions and Cloud Scheduler ensures that the environments are managed efficiently without manual intervention."
              }
            ],
            "answer": "B",
            "most_voted": "B",
            "page": 85,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56686-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "Cost-Effective Hadoop Job Migration",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You need to migrate Hadoop jobs for your company's Data Science team without modifying the underlying infrastructure. You want to minimize costs and infrastructure management effort. What should you do?",
            "choices": {
              "A": "Create a Dataproc cluster using standard worker instances.",
              "B": "Create a Dataproc cluster using preemptible worker instances.",
              "C": "Manually deploy a Hadoop cluster on Compute Engine using standard instances.",
              "D": "Manually deploy a Hadoop cluster on Compute Engine using preemptible instances."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "Cost Minimization",
                "keywords": ["Dataproc", "preemptible instances", "cost-effective", "Hadoop"],
                "explanation": "Using preemptible worker instances in a Dataproc cluster reduces costs significantly while still enabling the Data Science team to run Hadoop jobs efficiently."
              },
              {
                "title": "Infrastructure Management",
                "keywords": ["Dataproc", "managed service", "Hadoop jobs", "cloud infrastructure"],
                "explanation": "Dataproc simplifies infrastructure management by providing a managed service for running Hadoop jobs, reducing the operational overhead compared to manually deploying Hadoop clusters on Compute Engine."
              }
            ],
            "answer": "B",
            "most_voted": "B",
            "page": 86,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56684-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "Integration of Overlapping VPC Networks",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company has just acquired another company, and you have been asked to integrate their existing Google Cloud environment into your company's data center. Upon investigation, you discover that some of the RFC 1918 IP ranges being used in the new company's Virtual Private Cloud (VPC) overlap with your data center IP space. What should you do to enable connectivity and make sure that there are no routing conflicts when connectivity is established?",
            "choices": {
              "A": "Create a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply new IP addresses so there is no overlapping IP space.",
              "B": "Create a Cloud VPN connection from the new VPC to the data center, and create a Cloud NAT instance to perform NAT on the overlapping IP space.",
              "C": "Create a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply a custom route advertisement to block the overlapping IP space.",
              "D": "Create a Cloud VPN connection from the new VPC to the data center, and apply a firewall rule that blocks the overlapping IP space."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "Conflict Resolution in Network Integration",
                "keywords": ["VPC integration", "overlapping IP ranges", "Cloud VPN", "Cloud Router"],
                "explanation": "Reassigning IP addresses to eliminate overlaps ensures seamless integration of the acquired company's VPC into your company's data center without routing conflicts."
              },
              {
                "title": "Best Practice for Merging Networks",
                "keywords": ["network integration", "Google Cloud", "RFC 1918", "IP conflict resolution"],
                "explanation": "Reassigning conflicting IP ranges and using a Cloud Router to manage routes is the recommended approach to integrating networks with overlapping IP spaces."
              }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 87,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56680-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "API Versioning Strategy",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company provides a recommendation engine for retail customers. You are providing retail customers with an API where they can submit a user ID and the API returns a list of recommendations for that user. You are responsible for the API lifecycle and want to ensure stability for your customers in case the API makes backward-incompatible changes. You want to follow Google-recommended practices. What should you do?",
            "choices": {
              "A": "Create a distribution list of all customers to inform them of an upcoming backward-incompatible change at least one month before replacing the old API with the new API.",
              "B": "Create an automated process to generate API documentation, and update the public API documentation as part of the CI/CD process when deploying an update to the API.",
              "C": "Use a versioning strategy for the APIs that increases the version number on every backward-incompatible change.",
              "D": "Use a versioning strategy for the APIs that adds the suffix \u2018DEPRECATED\u2019 to the current API version number on every backward-incompatible change. Use the current version number for the new API."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "API Versioning for Stability",
                "keywords": ["API versioning", "backward-incompatible changes", "stability"],
                "explanation": "Incrementing the version number for every backward-incompatible change allows clients to continue using the older version while they adapt to the new one, ensuring stability."
              },
              {
                "title": "Best Practice in API Management",
                "keywords": ["API lifecycle", "CI/CD", "version control"],
                "explanation": "Following a structured versioning strategy is critical for maintaining API stability and minimizing disruption to clients."
              }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 88,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56656-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "Securing Connectivity in Google Cloud",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Mountkirk Games wants you to secure the connectivity from the new gaming application platform to Google Cloud. You want to streamline the process and follow Google-recommended practices. What should you do?",
            "choices": {
              "A": "Configure Workload Identity and service accounts to be used by the application platform.",
              "B": "Use Kubernetes Secrets, which are obfuscated by default. Configure these Secrets to be used by the application platform.",
              "C": "Configure Kubernetes Secrets to store the secret, enable Application-Layer Secrets Encryption, and use Cloud Key Management Service (Cloud KMS) to manage the encryption keys. Configure these Secrets to be used by the application platform.",
              "D": "Configure HashiCorp Vault on Compute Engine, and use customer managed encryption keys and Cloud Key Management Service (Cloud KMS) to manage the encryption keys. Configure these Secrets to be used by the application platform."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "Workload Identity",
                "keywords": ["Workload Identity", "service accounts", "secure connectivity"],
                "explanation": "Workload Identity allows workloads running on Google Kubernetes Engine to access Google Cloud services securely by using service accounts, following best practices for identity and access management."
              },
              {
                "title": "Streamlined Security Setup",
                "keywords": ["security practices", "Google Cloud", "service accounts"],
                "explanation": "Using Workload Identity and service accounts simplifies the process of securing connections between applications and Google Cloud services."
              }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 89,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56645-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "Testing Microservice Failure in GKE",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You are developing your microservices application on Google Kubernetes Engine. During testing, you want to validate the behavior of your application in case a specific microservice should suddenly crash. What should you do?",
            "choices": {
              "A": "Add a taint to one of the nodes of the Kubernetes cluster. For the specific microservice, configure a pod anti-affinity label that has the name of the tainted node as a value.",
              "B": "Use Istio's fault injection on the particular microservice whose faulty behavior you want to simulate.",
              "C": "Destroy one of the nodes of the Kubernetes cluster to observe the behavior.",
              "D": "Configure Istio's traffic management features to steer the traffic away from a crashing microservice."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "Fault Injection Testing",
                "keywords": ["Istio", "fault injection", "microservices", "GKE"],
                "explanation": "Using Istio's fault injection allows you to simulate various failure scenarios for a microservice, helping you understand and validate how your application handles crashes."
              },
              {
                "title": "Resilience Testing",
                "keywords": ["resilience testing", "microservices", "Kubernetes"],
                "explanation": "Simulating microservice failures is essential for testing the resilience of your application in a controlled environment."
              }
            ],
            "answer": "B",
            "most_voted": "B",
            "page": 90,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56640-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "Building a Cost-Effective and Reliable Web Application",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company wants you to build a highly reliable web application with a few public APIs as the backend. You don't expect a lot of user traffic, but traffic could spike occasionally. You want to leverage Cloud Load Balancing, and the solution must be cost-effective for users. What should you do?",
            "choices": {
              "A": "Store static content such as HTML and images in Cloud CDN. Host the APIs on App Engine and store the user data in Cloud SQL.",
              "B": "Store static content such as HTML and images in a Cloud Storage bucket. Host the APIs on a zonal Google Kubernetes Engine cluster with worker nodes in multiple zones, and save the user data in Cloud Spanner.",
              "C": "Store static content such as HTML and images in Cloud CDN. Use Cloud Run to host the APIs and save the user data in Cloud SQL.",
              "D": "Store static content such as HTML and images in a Cloud Storage bucket. Use Cloud Functions to host the APIs and save the user data in Firestore."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "Cost-Effective Hosting",
                "keywords": ["cost-effective", "Cloud Storage", "Cloud Functions", "Firestore"],
                "explanation": "Using Cloud Functions for hosting APIs and Firestore for storing data offers a cost-effective solution, especially for workloads with occasional traffic spikes."
              },
              {
                "title": "High Reliability with Cloud Load Balancing",
                "keywords": ["Cloud Load Balancing", "scalability", "reliability", "Cloud Functions"],
                "explanation": "Cloud Functions, paired with Cloud Load Balancing, ensures that the application can handle traffic spikes while maintaining reliability."
              }
            ],
            "answer": "D",
            "most_voted": "D",
            "page": 91,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56615-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "High-Volume Web Service Platform Selection",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You are implementing the infrastructure for a web service on Google Cloud. The web service needs to receive and store the data from 500,000 requests per second. The data will be queried later in real time, based on exact matches of a known set of attributes. There will be periods where the web service will not receive any requests. The business wants to keep costs low. Which web service platform and database should you use for the application?",
            "choices": {
              "A": "Cloud Run and BigQuery",
              "B": "Cloud Run and Cloud Bigtable",
              "C": "A Compute Engine autoscaling managed instance group and BigQuery",
              "D": "A Compute Engine autoscaling managed instance group and Cloud Bigtable"
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "High-Throughput Data Ingestion",
                "keywords": ["Cloud Run", "Cloud Bigtable", "high-throughput", "scalable"],
                "explanation": "Cloud Run allows you to scale out to handle the high request rate, while Cloud Bigtable efficiently manages high-throughput data storage and retrieval."
              },
              {
                "title": "Cost Management",
                "keywords": ["cost-effective", "serverless", "Cloud Run", "Cloud Bigtable"],
                "explanation": "Using serverless technologies like Cloud Run ensures that costs are minimized during periods of low activity, while Cloud Bigtable offers a cost-effective solution for real-time queries."
              }
            ],
            "answer": "B",
            "most_voted": "B",
            "page": 92,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56612-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "Autoscaling and Performance Troubleshooting",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your operations team has asked you to help diagnose a performance issue in a production application that runs on Compute Engine. The application is dropping requests that reach it when under heavy load. The process list for affected instances shows a single application process that is consuming all available CPU, and autoscaling has reached the upper limit of instances. There is no abnormal load on any other related systems, including the database. You want to allow production traffic to be served again as quickly as possible. Which action should you recommend?",
            "choices": {
              "A": "Change the autoscaling metric to agent.googleapis.com/memory/percent_used.",
              "B": "Restart the affected instances on a staggered schedule.",
              "C": "SSH to each instance and restart the application process.",
              "D": "Increase the maximum number of instances in the autoscaling group."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "Scaling Limits",
                "keywords": ["autoscaling", "Compute Engine", "scalability", "performance"],
                "explanation": "Increasing the maximum number of instances in the autoscaling group allows more resources to be allocated to handle the increased load, restoring the application's ability to serve requests."
              },
              {
                "title": "Quick Resolution",
                "keywords": ["production traffic", "quick resolution", "Compute Engine"],
                "explanation": "By increasing the instance limit, the application can immediately begin serving more traffic, preventing further request drops."
              }
            ],
            "answer": "D",
            "most_voted": "D",
            "page": 93,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56603-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "Restricting External IP Usage",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your organization has decided to restrict the use of external IP addresses on instances to only approved instances. You want to enforce this requirement across all of your Virtual Private Clouds (VPCs). What should you do?",
            "choices": {
              "A": "Remove the default route on all VPCs. Move all approved instances into a new subnet that has a default route to an internet gateway.",
              "B": "Create a new VPC in custom mode. Create a new subnet for the approved instances, and set a default route to the internet gateway on this new subnet.",
              "C": "Implement a Cloud NAT solution to remove the need for external IP addresses entirely.",
              "D": "Set an Organization Policy with a constraint on constraints/compute.vmExternalIpAccess. List the approved instances in the allowedValues list."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "Policy Enforcement",
                "keywords": ["Organization Policy", "external IP", "security", "compute.vmExternalIpAccess"],
                "explanation": "Using an Organization Policy with the compute.vmExternalIpAccess constraint ensures that only approved instances are allowed to use external IPs, enforcing security policies across all VPCs."
              },
              {
                "title": "Scalability and Consistency",
                "keywords": ["policy management", "cloud governance", "VPC", "external IPs"],
                "explanation": "An Organization Policy allows for scalable and consistent enforcement of security measures across the entire organization."
              }
            ],
            "answer": "D",
            "most_voted": "D",
            "page": 94,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56576-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "Monitoring and Diagnosing GKE Applications",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You have an application that runs in Google Kubernetes Engine (GKE). Over the last 2 weeks, customers have reported that a specific part of the application returns errors very frequently. You currently have no logging or monitoring solution enabled on your GKE cluster. You want to diagnose the problem, but you have not been able to replicate the issue. You want to cause minimal disruption to the application. What should you do?",
            "choices": {
              "A": "1. Update your GKE cluster to use Cloud Operations for GKE. 2. Use the GKE Monitoring dashboard to investigate logs from affected Pods.",
              "B": "1. Create a new GKE cluster with Cloud Operations for GKE enabled. 2. Migrate the affected Pods to the new cluster, and redirect traffic for those Pods to the new cluster. 3. Use the GKE Monitoring dashboard to investigate logs from affected Pods.",
              "C": "1. Update your GKE cluster to use Cloud Operations for GKE, and deploy Prometheus. 2. Set an alert to trigger whenever the application returns an error.",
              "D": "1. Create a new GKE cluster with Cloud Operations for GKE enabled, and deploy Prometheus. 2. Migrate the affected Pods to the new cluster, and redirect traffic for those Pods to the new cluster. 3. Set an alert to trigger whenever the application returns an error."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "Minimal Disruption",
                "keywords": ["GKE", "Cloud Operations", "monitoring", "logging"],
                "explanation": "Updating the existing GKE cluster to use Cloud Operations ensures minimal disruption while allowing you to diagnose and resolve the issue."
              },
              {
                "title": "Effective Monitoring",
                "keywords": ["GKE Monitoring", "Cloud Operations", "troubleshooting"],
                "explanation": "Using the GKE Monitoring dashboard provides insights into logs and metrics, helping identify and resolve errors without impacting the production environment."
              }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 95,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56425-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "VPC Peering for Inter-VPC Communication",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company has a project in Google Cloud with three Virtual Private Clouds (VPCs). There is a Compute Engine instance on each VPC. Network subnets do not overlap and must remain separated. The network configuration is shown below. Instance #1 is an exception and must communicate directly with both Instance #2 and Instance #3 via internal IPs. How should you accomplish this?",
            "choices": {
              "A": "Create a cloud router to advertise subnet #2 and subnet #3 to subnet #1.",
              "B": "Add two additional NICs to Instance #1 with the following configuration: NIC1 - VPC: VPC #2 - SUBNETWORK: subnet #2, NIC2 - VPC: VPC #3 - SUBNETWORK: subnet #3. Update firewall rules to enable traffic between instances.",
              "C": "Create two VPN tunnels via CloudVPN: 1 between VPC #1 and VPC #2, and 1 between VPC #2 and VPC #3. Update firewall rules to enable traffic between the instances.",
              "D": "Peer all three VPCs: Peer VPC #1 with VPC #2, Peer VPC #2 with VPC #3. Update firewall rules to enable traffic between the instances."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "Direct Communication Between VPCs",
                "keywords": ["VPC Peering", "internal IP", "Google Cloud", "subnets"],
                "explanation": "Adding additional NICs to Instance #1 and configuring them for VPC #2 and VPC #3 allows for direct communication between the instances without violating subnet separation."
              },
              {
                "title": "Traffic Routing and Security",
                "keywords": ["firewall rules", "networking", "multi-VPC communication"],
                "explanation": "Updating firewall rules ensures secure and controlled traffic flow between the instances, which is critical in a multi-VPC setup."
              }
            ],
            "answer": "B",
            "most_voted": "B",
            "page": 96,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56416-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "Zonal Outage Recovery in Compute Engine",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "Your company is designing its application landscape on Compute Engine. Whenever a zonal outage occurs, the application should be restored in another zone as quickly as possible with the latest application data. You need to design the solution to meet this requirement. What should you do?",
            "choices": {
              "A": "Create a snapshot schedule for the disk containing the application data. Whenever a zonal outage occurs, use the latest snapshot to restore the disk in the same zone.",
              "B": "Configure the Compute Engine instances with an instance template for the application, and use a regional persistent disk for the application data. Whenever a zonal outage occurs, use the instance template to spin up the application in another zone in the same region. Use the regional persistent disk for the application data.",
              "C": "Create a snapshot schedule for the disk containing the application data. Whenever a zonal outage occurs, use the latest snapshot to restore the disk in another zone within the same region.",
              "D": "Configure the Compute Engine instances with an instance template for the application, and use a regional persistent disk for the application data. Whenever a zonal outage occurs, use the instance template to spin up the application in another region. Use the regional persistent disk for the application data."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "Zonal Outage Recovery",
                "keywords": ["Compute Engine", "zonal outage", "regional persistent disk"],
                "explanation": "Using regional persistent disks ensures that data is replicated across zones, allowing for a quick recovery by spinning up the application in another zone within the same region."
              },
              {
                "title": "Application Availability",
                "keywords": ["high availability", "instance template", "Compute Engine"],
                "explanation": "Instance templates allow for the rapid deployment of instances in a different zone, minimizing downtime and maintaining application availability during zonal outages."
              }
            ],
            "answer": "B",
            "most_voted": "B",
            "page": 97,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56403-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "Rolling Updates in Managed Instance Groups",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You have developed a non-critical update to your application that is running in a managed instance group, and have created a new instance template with the update that you want to release. To prevent any possible impact to the application, you don't want to update any running instances. You want any new instances that are created by the managed instance group to contain the new update. What should you do?",
            "choices": {
              "A": "Start a new rolling restart operation.",
              "B": "Start a new rolling replace operation.",
              "C": "Start a new rolling update. Select the Proactive update mode.",
              "D": "Start a new rolling update. Select the Opportunistic update mode."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "Safe Application Updates",
                "keywords": ["rolling update", "managed instance group", "Compute Engine"],
                "explanation": "Selecting the Opportunistic update mode allows new instances to be updated while avoiding updates to running instances, reducing the risk of impacting the production environment."
              },
              {
                "title": "Controlled Instance Management",
                "keywords": ["instance template", "Compute Engine", "instance group"],
                "explanation": "This approach ensures that only new instances created by the managed instance group will use the new template, providing a safe and controlled way to deploy non-critical updates."
              }
            ],
            "answer": "D",
            "most_voted": "D",
            "page": 98,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56399-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "Deploying Stateful Workloads on Google Cloud",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You need to deploy a stateful workload on Google Cloud. The workload can scale horizontally, but each instance needs to read and write to the same POSIX filesystem. At high load, the stateful workload needs to support up to 100 MB/s of writes. What should you do?",
            "choices": {
              "A": "Use a persistent disk for each instance.",
              "B": "Use a regional persistent disk for each instance.",
              "C": "Create a Cloud Filestore instance and mount it in each instance.",
              "D": "Create a Cloud Storage bucket and mount it in each instance using gcsfuse."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "Stateful Workload Requirements",
                "keywords": ["stateful workload", "Cloud Filestore", "POSIX filesystem"],
                "explanation": "Cloud Filestore provides a managed file storage service that supports POSIX-compliant file systems, making it suitable for stateful workloads requiring shared access across instances."
              },
              {
                "title": "High Throughput Support",
                "keywords": ["100 MB/s", "Cloud Filestore", "high throughput"],
                "explanation": "Cloud Filestore is designed to handle high-throughput workloads, offering the necessary performance for workloads requiring 100 MB/s of write operations."
              }
            ],
            "answer": "C",
            "most_voted": "C",
            "page": 99,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56384-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          },
          {
            "source": "",
            "topic": "Designing for Data Deletion Compliance",
            "classification": "[All Professional Cloud Architect Questions]",
            "question": "You are working at a sports association whose members range in age from 8 to 30. The association collects a large amount of health data, such as sustained injuries. You are storing this data in BigQuery. Current legislation requires you to delete such information upon request of the subject. You want to design a solution that can accommodate such a request. What should you do?",
            "choices": {
              "A": "Use a unique identifier for each individual. Upon a deletion request, delete all rows from BigQuery with this identifier.",
              "B": "When ingesting new data in BigQuery, run the data through the Data Loss Prevention (DLP) API to identify any personal information. As part of the DLP scan, save the result to Data Catalog. Upon a deletion request, query Data Catalog to find the column with personal information.",
              "C": "Create a BigQuery view over the table that contains all data. Upon a deletion request, exclude the rows that affect the subject's data from this view. Use this view instead of the source table for all analysis tasks.",
              "D": "Use a unique identifier for each individual. Upon a deletion request, overwrite the column with the unique identifier with a salted SHA256 of its value."
            },
            "type": "option",
            "keyPoints": [
              {
                "title": "Data Deletion Compliance",
                "keywords": ["data deletion", "compliance", "BigQuery", "unique identifier"],
                "explanation": "Using a unique identifier allows you to easily locate and delete specific records in BigQuery, ensuring compliance with legislation that requires data removal upon request."
              },
              {
                "title": "Legislation and Privacy",
                "keywords": ["privacy", "data protection", "BigQuery", "data deletion"],
                "explanation": "This approach ensures that the organization can comply with data privacy regulations by accurately and efficiently removing the required data from BigQuery."
              }
            ],
            "answer": "A",
            "most_voted": "A",
            "page": 100,
            "URL": [
              "https://www.examtopics.com/discussions/google/view/56381-exam-professional-cloud-architect-topic-1-question-126/"
            ]
          }
    ]
}